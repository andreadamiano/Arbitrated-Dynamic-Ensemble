{"cells":[{"cell_type":"markdown","source":["#Arbitrated Dynamic Ensemble"],"metadata":{"id":"YCqB1p6ZJbqU"}},{"cell_type":"markdown","source":["##Get Data"],"metadata":{"id":"vUfBVn182zxU"}},{"cell_type":"code","source":["#install libraries\n","!pip install --upgrade linear-tree"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FB22Z6Gr7ClM","executionInfo":{"status":"ok","timestamp":1740609785587,"user_tz":-60,"elapsed":3346,"user":{"displayName":"Andrea Damiano","userId":"07680645090793871056"}},"outputId":"86ffddda-a3c9-445d-a6f0-5a2be00eecf0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting linear-tree\n","  Downloading linear_tree-0.3.5-py3-none-any.whl.metadata (8.0 kB)\n","Requirement already satisfied: scikit-learn>=0.24.2 in /usr/local/lib/python3.11/dist-packages (from linear-tree) (1.6.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from linear-tree) (1.26.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from linear-tree) (1.13.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.2->linear-tree) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.2->linear-tree) (3.5.0)\n","Downloading linear_tree-0.3.5-py3-none-any.whl (21 kB)\n","Installing collected packages: linear-tree\n","Successfully installed linear-tree-0.3.5\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tHIikL4sotXw"},"outputs":[],"source":["#import libraries\n","import pandas as pd\n","import numpy as np\n","from google.colab import drive\n","import datetime\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","import statsmodels.api as sm\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","import shap\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from torch.utils.data import DataLoader\n","from joblib import Parallel, delayed, dump, load\n","import xgboost as xgb\n","import time\n","import yfinance as yf\n","import torch.nn.functional as F\n","from sklearn.preprocessing import RobustScaler\n","from sklearn.linear_model import LinearRegression\n","from lineartree import LinearBoostRegressor, LinearBoostClassifier\n","from sklearn.datasets import make_regression\n","from sklearn.linear_model import Ridge\n","\n","\n","#ignore warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","\n","#global formatting for the plots\n","plt.rcParams.update({\n","    'axes.titlesize': 18,    # Title font size\n","    'axes.labelsize': 14,    # x/y label font size\n","    'xtick.labelsize': 12,   # x-axis tick label size\n","    'ytick.labelsize': 12,   # y-axis tick label size\n","    'legend.fontsize': 12,   # Legend text size\n","    'legend.title_fontsize': 13  # Legend title size\n","})"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27119,"status":"ok","timestamp":1740609820935,"user":{"displayName":"Andrea Damiano","userId":"07680645090793871056"},"user_tz":-60},"id":"aHYVviIiqofN","outputId":"132c1237-94ad-48d2-cea2-b1ae39cb5b93"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["drive.mount('/content/gdrive')\n","\n","#import time series\n","datasets = {}\n","for name in ['sp500', 'eur_usd']:\n","    datasets[name] = pd.read_csv(f'/content/gdrive/MyDrive/università/Machine_learning/Project_code/Datasets/{name}_data.csv', index_col=0)\n","    datasets[name] = datasets[name]\n","    datasets[name].index.name = 'Date'\n","    datasets[name].index = pd.to_datetime(datasets[name].index)\n","\n","\n","Close_price = {}\n","Log_return = {}\n","test_residuals = {}\n","val_residuals = {}\n","SARIMA_fitted_values = {}\n","SARIMA_test_predictions = {}\n","residuals = {}\n","\n","for name , dataset in datasets.items():\n","\n","    #save Close price and Log Returns into a pd.Series\n","    Close_price[name] = dataset['Close'].copy().astype(float)\n","    # Log_return[name] = dataset['Log Return'].copy().astype(float)\n","\n","    #drop columns not needed\n","    # dataset.drop(['Open', 'High', 'Low', 'Close', 'Log Return'], axis=1, inplace=True)\n","    dataset.drop(['Open', 'High', 'Low', 'Close'], axis=1, inplace=True)\n","\n","\n","    #import SARIMA residuals\n","    residuals[name] = pd.read_csv(f'/content/gdrive/MyDrive/università/Machine_learning/Project_code/ARIMA/{name}/SARIMA_residuals.csv', index_col=0, sep=\",\")\n","    residuals[name].index = pd.to_datetime(residuals[name].index)\n","\n","\n","    #val residuals\n","    val_residuals[name] = pd.read_csv(f'/content/gdrive/MyDrive/università/Machine_learning/Project_code/ARIMA/{name}/SARIMA_val_residuals.csv', index_col=0, sep=\",\")\n","    val_residuals[name].index = pd.to_datetime(val_residuals[name].index)\n","\n","    #test residuals\n","    test_residuals[name] = pd.read_csv(f'/content/gdrive/MyDrive/università/Machine_learning/Project_code/ARIMA/{name}/SARIMA_test_residuals.csv', index_col=0, sep=\",\")\n","    test_residuals[name].index = pd.to_datetime(test_residuals[name].index)\n","\n","    #import SARIMA fitted values (predictions on the training set)\n","    SARIMA_fitted_values[name] = pd.read_csv(f'/content/gdrive/MyDrive/università/Machine_learning/Project_code/ARIMA/{name}/SARIMA_fitted_values.csv', index_col=0)\n","    SARIMA_fitted_values[name].index = pd.to_datetime(SARIMA_fitted_values[name].index)\n","\n","    #import SARIMA predictions on the test set\n","    SARIMA_test_predictions[name] = pd.read_csv(f'/content/gdrive/MyDrive/università/Machine_learning/Project_code/ARIMA/{name}/SARIMA_test_predictions.csv', index_col=0).squeeze() #to read as a pd.Series\n","    SARIMA_test_predictions[name].index = pd.to_datetime(SARIMA_test_predictions[name].index)"]},{"cell_type":"markdown","metadata":{"id":"kUTM7c1K61Gw"},"source":["###Helper functions to move data to the GPU"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PZ-clsO861OU"},"outputs":[],"source":["#define some helper classes\n","def get_device():\n","    if torch.cuda.is_available():\n","        device = 'cuda'\n","    else:\n","        device = 'cpu'\n","\n","    return device\n","\n","def to_device(data, device):\n","    if isinstance(data, (list, tuple)):\n","        return [to_device(x, device) for x in data]\n","    elif isinstance(data, torch.Tensor):  # Only move tensors to the device\n","        return data.to(device, non_blocking=True)\n","    else:\n","        return data  # For non-tensor types (e.g., strings), return as is\n","\n","\n","class DeviceDataLoader (): #receive a dataloader and move to the correct device\n","  def __init__(self, dl, device):\n","    self.dl = dl\n","    self.device = device\n","\n","  def __iter__(self):\n","    for batch in self.dl:\n","      yield to_device(batch, device)\n","\n","  def __len__(self):\n","    return len(self.dl)\n","\n","\n","#get device\n","device = get_device()"]},{"cell_type":"markdown","metadata":{"id":"x9kDTw_c65xs"},"source":["###Get pre-trained models"]},{"cell_type":"markdown","source":["LSTM followed by CNN"],"metadata":{"id":"lSAxWrKSOGeo"}},{"cell_type":"code","source":["#custom loss\n","class RMSELoss(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.mse = nn.MSELoss()\n","\n","    def forward(self, yhat, y):\n","        return torch.sqrt(self.mse(yhat, y))\n","\n","\n","#istance of the custom loss\n","rmse_loss = torch.nn.MSELoss().to(device) #the loss is computed off the mse loss, the function return the rmse for easier comprehension\n","\n","\n","class LSTM_CNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n","        super().__init__() #initialize the parent class\n","        self.num_classes = num_classes\n","        self.num_layers = num_layers\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","\n","        self.lstm = nn.LSTM(input_size=input_size,\n","                            hidden_size=hidden_size,\n","                            num_layers=num_layers,\n","                            batch_first=True) #This argument specifies the input and output tensors are provided as (batch, seq, feature)\n","        self.cnn = nn.Sequential(\n","            nn.Conv1d(in_channels=hidden_size, out_channels=64, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool1d(kernel_size=2, stride=2),\n","            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool1d(kernel_size=2, stride=2),\n","            #flatten\n","            nn.Flatten(),\n","            nn.LazyLinear(out_features=256),\n","            nn.ReLU(),\n","            nn.Linear(in_features=256, out_features=num_classes)\n","        )\n","\n","    def forward(self, x):\n","        out, _ = self.lstm(x)  #stateless LSTM each batch is using a different hidden/cell state (initialized to 0)\n","        out = out.permute(0, 2, 1)\n","        out = self.cnn(out)\n","        # out = out.squeeze(-1)\n","        # print(out)\n","        return out\n","\n","    def training_step(self, batch):\n","        x, y = batch\n","        out = self(x) # This calls self.forward(x) through the __call__ method\n","        #loss = torch.sqrt(F.mse_loss(out, y)) incorrect way\n","        loss = rmse_loss(out, y)\n","        return loss\n","\n","    def validation_step(self, batch):\n","        x, y = batch\n","        out = self(x)\n","        #loss = torch.sqrt(F.mse_loss(out, y))\n","        loss = rmse_loss(out, y)\n","        return {'val_loss': loss.detach()} #loss.detatch disable gradient computation\n","\n","    def validation_epoch_end(self, outputs):\n","        batch_losses = [x['val_loss'] for x in outputs]\n","        epoch_loss = torch.stack(batch_losses).mean()\n","        return {'val_loss': epoch_loss.item()}\n","\n","    def epoch_end(self, epoch, result):\n","        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}\".format(epoch, result['lrs'][-1], result['train_loss'], result['val_loss']))\n","\n","\n","#instance of the model\n","LSTM_CNN_model = {}\n","\n","for name, dataset in datasets.items():\n","\n","    num_classes = 1 # regression\n","    # input_size = 32  # Number of features per time step\n","    input_size = 1 #number of input features\n","    hidden_size = 512 #number of hidden layer in each cell, the more is better, but also will slow down the training\n","    num_layers = 1\n","\n","    LSTM_CNN_model[name] = LSTM_CNN(num_classes= num_classes, input_size = input_size, hidden_size = hidden_size, num_layers = num_layers)\n","    LSTM_CNN_model[name].to(device) #move the istance to the device"],"metadata":{"id":"eO8TnpelOGeo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["CNN + LSTM parallel architecture"],"metadata":{"id":"gTsHRU23OGep"}},{"cell_type":"code","source":["class ParallelCNN_LSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n","        super().__init__()\n","        self.cnn = nn.Sequential(\n","            nn.Conv1d(in_channels=input_size, out_channels=64, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool1d(kernel_size=2, stride=2),\n","            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool1d(kernel_size=2, stride=2),\n","            nn.Flatten(),\n","            nn.LazyLinear(out_features=128), #linear layer that automatically infer the input size\n","            nn.ReLU()\n","        )\n","        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n","        self.fc_lstm = nn.Linear(hidden_size, 128)\n","        self.fc = nn.Linear(128*2, num_classes)\n","\n","    def forward(self, x):\n","        #cnn takes input of shape (batch_size, channels, seq_len)\n","        x_cnn = x.permute(0, 2, 1)\n","        out_cnn = self.cnn(x_cnn)\n","        # lstm takes input of shape (batch_size, seq_len, input_size)\n","        out_lstm, _ = self.lstm(x)\n","        out_lstm = self.fc_lstm(out_lstm[:, -1, :])\n","        out = torch.cat([out_cnn, out_lstm], dim=1)\n","        out = self.fc(out)\n","        return out\n","\n","    def training_step(self, batch):\n","        x, y = batch\n","        out = self(x) # This calls self.forward(x) through the __call__ method\n","        #loss = torch.sqrt(F.mse_loss(out, y)) incorrect way\n","        loss = rmse_loss(out, y)\n","        return loss\n","\n","    def validation_step(self, batch):\n","        x, y = batch\n","        out = self(x)\n","        #loss = torch.sqrt(F.mse_loss(out, y))\n","        loss = rmse_loss(out, y)\n","        return {'val_loss': loss.detach()} #loss.detatch disable gradient computation\n","\n","    def validation_epoch_end(self, outputs):\n","        batch_losses = [x['val_loss'] for x in outputs]\n","        epoch_loss = torch.stack(batch_losses).mean()\n","        return {'val_loss': epoch_loss.item()}\n","\n","    def epoch_end(self, epoch, result):\n","        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}\".format(epoch, result['lrs'][-1], result['train_loss'], result['val_loss']))\n","\n","\n","#instance of the model\n","parallelCNNLSTM_model = {}\n","\n","for name, dataset in datasets.items():\n","\n","    num_classes = 1 # regression\n","    # input_size = 32  # Number of features per time step\n","    input_size = 1 #number of input features\n","    hidden_size = 512 #number of hidden layer in each cell, the more is better, but also will slow down the training\n","    num_layers = 1\n","\n","    parallelCNNLSTM_model[name] = ParallelCNN_LSTM(num_classes= num_classes, input_size = input_size, hidden_size = hidden_size, num_layers = num_layers)\n","    parallelCNNLSTM_model[name].to(device) #move the istance to the device"],"metadata":{"id":"YdW-Jc_oOGep"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["LSTM"],"metadata":{"id":"rjxEJFhjOQ3W"}},{"cell_type":"code","source":["#custom loss\n","class RMSELoss(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.mse = nn.MSELoss()\n","\n","    def forward(self, yhat, y):\n","        return torch.sqrt(self.mse(yhat, y))\n","\n","\n","#istance of the custom loss\n","rmse_loss = torch.nn.MSELoss().to(device) #the loss is computed off the mse loss, the function return the rmse for easier comprehension\n","\n","class LSTM(nn.Module):\n","\n","    def __init__(self, num_classes, input_size, hidden_size=512, num_layers=3, dropout=0.3):\n","        super().__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.num_classes = num_classes\n","\n","        # LSTM layers\n","        self.lstm = nn.LSTM(\n","            input_size=input_size,\n","            hidden_size=hidden_size,\n","            num_layers=num_layers,\n","            batch_first=True,\n","            dropout=dropout\n","        )\n","\n","        # Fully connected layers\n","        self.fc1 = nn.Linear(hidden_size, 256)\n","        self.fc2 = nn.Linear(256, 128)\n","        self.fc3 = nn.Linear(128, self.num_classes)\n","\n","        # Activation and dropout\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","    def forward(self, x):\n","        # LSTM output\n","        lstm_out, _ = self.lstm(x)\n","\n","        # Pass only the last output\n","        lstm_out = lstm_out[:, -1, :]\n","\n","        # Fully connected layers with ReLU and Dropout\n","        x = self.relu(self.fc1(lstm_out))\n","        x = self.dropout(x)\n","        x = self.relu(self.fc2(x))\n","        x = self.dropout(x)\n","        x = self.fc3(x)  # Final prediction\n","        return x\n","\n","    def training_step(self, batch):\n","        x, y = batch\n","        out = self(x) # This calls self.forward(x) through the __call__ method\n","        #loss = torch.sqrt(F.mse_loss(out, y)) incorrect way\n","        loss = rmse_loss(out, y)\n","        return loss\n","\n","    def validation_step(self, batch):\n","        x, y = batch\n","        out = self(x)\n","        #loss = torch.sqrt(F.mse_loss(out, y))\n","        loss = rmse_loss(out, y)\n","        return {'val_loss': loss.detach()} #loss.detatch disable gradient computation\n","\n","    def validation_epoch_end(self, outputs):\n","        batch_losses = [x['val_loss'] for x in outputs]\n","        epoch_loss = torch.stack(batch_losses).mean()\n","        return {'val_loss': epoch_loss.item()}\n","\n","    def epoch_end(self, epoch, result):\n","        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}\".format(epoch, result['lrs'][-1], result['train_loss'], result['val_loss']))\n","\n","\n","#instance of the model\n","LSTM_model = {}\n","\n","for name, dataset in datasets.items():\n","\n","    num_classes = 1 # regression\n","    # input_size = 32  # Number of features per time step\n","    input_size = 1 #number of input features\n","    hidden_size = 512 #number of hidden layer in each cell, the more is better, but also will slow down the training\n","    num_layers = 3\n","\n","    LSTM_model[name] = LSTM(num_classes= num_classes, input_size = input_size, hidden_size = hidden_size, num_layers = num_layers)\n","    LSTM_model[name].to(device) #move the istance to the device"],"metadata":{"id":"rIr7-Y48OGeq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["CNN model"],"metadata":{"id":"SXxENhbpOGer"}},{"cell_type":"code","source":["class CNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n","        super().__init__()\n","        self.cnn = nn.Sequential(\n","            nn.Conv1d(in_channels=input_size, out_channels=64, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool1d(kernel_size=2, stride=2),\n","            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool1d(kernel_size=2, stride=2),\n","            nn.Flatten(),\n","            nn.LazyLinear(out_features=128), #linear layer that automatically infer the input size\n","            nn.ReLU()\n","        )\n","        self.fc = nn.Linear(128, num_classes)\n","\n","    def forward(self, x):\n","        #cnn takes input of shape (batch_size, channels, seq_len)\n","        x_cnn = x.permute(0, 2, 1)\n","        out_cnn = self.cnn(x_cnn)\n","        out = self.fc(out_cnn)\n","        return out\n","\n","    def training_step(self, batch):\n","        x, y = batch\n","        out = self(x) # This calls self.forward(x) through the __call__ method\n","        #loss = torch.sqrt(F.mse_loss(out, y)) incorrect way\n","        loss = rmse_loss(out, y)\n","        return loss\n","\n","    def validation_step(self, batch):\n","        x, y = batch\n","        out = self(x)\n","        #loss = torch.sqrt(F.mse_loss(out, y))\n","        loss = rmse_loss(out, y)\n","        return {'val_loss': loss.detach()} #loss.detatch disable gradient computation\n","\n","    def validation_epoch_end(self, outputs):\n","        batch_losses = [x['val_loss'] for x in outputs]\n","        epoch_loss = torch.stack(batch_losses).mean()\n","        return {'val_loss': epoch_loss.item()}\n","\n","    def epoch_end(self, epoch, result):\n","        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}\".format(epoch, result['lrs'][-1], result['train_loss'], result['val_loss']))\n","\n","\n","#instance of the model\n","CNN_model = {}\n","\n","for name, dataset in datasets.items():\n","\n","    num_classes = 1 # regression\n","    # input_size = 32  # Number of features per time step\n","    input_size = 1 #number of input features\n","    hidden_size = 512 #number of hidden layer in each cell, the more is better, but also will slow down the training\n","    num_layers = 1\n","\n","    CNN_model[name] = CNN(num_classes= num_classes, input_size = input_size, hidden_size = hidden_size, num_layers = num_layers)\n","    CNN_model[name].to(device) #move the istance to the device"],"metadata":{"id":"j0jPsYbFOGer"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pG_vXOzEbgOj"},"source":["Load models\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8496,"status":"ok","timestamp":1740609830093,"user":{"displayName":"Andrea Damiano","userId":"07680645090793871056"},"user_tz":-60},"id":"wttbvc7m5NjO","outputId":"b043ade9-33b2-427a-c47e-4ed6c456c01a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["# from inspect import modulesbyfile\n","drive.mount(\"/content/gdrive\")\n","\n","Linear_booster_model = {}\n","#import trained models\n","model_names = [\"Linear_booster.joblib\", \"CNN\", \"LSTM\", \"LSTM_CNN\", \"ParallelCNN_LSTM\"]\n","model_istances = [\"Linear_booster_model\", CNN_model, LSTM_model, LSTM_CNN_model, parallelCNNLSTM_model]\n","\n","\n","for name, _ in datasets.items():\n","    for i, model in enumerate(model_names):\n","        if model == \"Linear_booster.joblib\":\n","            model_path = f'/content/gdrive/MyDrive/università/Tesi/models/{name}/{model}'\n","            Linear_booster_model[name] = load(model_path)\n","        else:\n","            #load state dict\n","            model_path = f'/content/gdrive/MyDrive/università/Tesi/models/{name}/{model}_fine_tuned'\n","            model_istances[i][name].load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n"]},{"cell_type":"markdown","metadata":{"id":"Ou6zXXECJT3j"},"source":["##ADE library"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aEI9f9NhJTa0","collapsed":true},"outputs":[],"source":["#import libraries\n","import pandas as pd\n","import numpy as np\n","from google.colab import drive\n","import datetime\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","import statsmodels.api as sm\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from joblib import Parallel, delayed, dump, load\n","import xgboost as xgb\n","import time\n","import yfinance as yf\n","import torch.nn.functional as F\n","from concurrent.futures import ThreadPoolExecutor\n","import threading\n","\n","#suppress warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","\n","class OverlappingTimeSeriesSplit():\n","    def __init__(self, n_splits=5, window_size=10):\n","        self.n_splits = n_splits\n","        self.window_size = window_size\n","\n","    def split(self, X):\n","        n_samples = len(X)\n","        fold_size = n_samples // (self.n_splits + 1)  # Ensure proper splitting\n","\n","        for i in range(1, self.n_splits + 1):  # Start from 1 to avoid empty train set\n","            train_end = i * fold_size\n","            train_idx = range(train_end)  # Train set includes all data before validation\n","\n","            val_start = max(0, train_end - self.window_size)\n","            val_end = train_end + fold_size\n","            val_idx = range(val_start, min(val_end, n_samples))  # Ensure within bounds\n","\n","            yield list(train_idx), list(val_idx)\n","\n","\n","def smape(y_true, y_pred):\n","    y_true = np.array(y_true)\n","    y_pred = np.array(y_pred)\n","\n","    numerator = np.abs(y_pred - y_true)\n","    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n","    smape_value = np.mean(numerator / denominator)  # Renamed variable to avoid conflict\n","    return smape_value\n","\n","#deep learning meta model architecture\n","class LSTM(nn.Module):\n","\n","    def __init__(self, num_classes, input_size, hidden_size=512, num_layers=3, dropout=0.3):\n","        super().__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.num_classes = num_classes\n","\n","        # LSTM layers\n","        self.lstm = nn.LSTM(\n","            input_size=input_size,\n","            hidden_size=hidden_size,\n","            num_layers=num_layers,\n","            batch_first=True,\n","            dropout=dropout\n","        )\n","\n","        # Fully connected layers\n","        self.fc1 = nn.Linear(hidden_size, 256)\n","        self.fc2 = nn.Linear(256, 128)\n","        self.fc3 = nn.Linear(128, self.num_classes)\n","\n","        # Activation and dropout\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","    def forward(self, x):\n","        # LSTM output\n","        lstm_out, _ = self.lstm(x)\n","\n","        # Pass only the last output\n","        lstm_out = lstm_out[:, -1, :]\n","\n","        # Fully connected layers with ReLU and Dropout\n","        x = self.relu(self.fc1(lstm_out))\n","        x = self.dropout(x)\n","        x = self.relu(self.fc2(x))\n","        x = self.dropout(x)\n","        x = self.fc3(x)  # Final prediction\n","        return x\n","\n","\n","#deep learning meta model architecture\n","class ParallelCNN_LSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n","        super().__init__()\n","        self.cnn = nn.Sequential(\n","            nn.Conv1d(in_channels=input_size, out_channels=64, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool1d(kernel_size=2, stride=2),\n","            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool1d(kernel_size=2, stride=2),\n","            nn.Flatten(),\n","            nn.LazyLinear(out_features=128), #linear layer that automatically infer the input size\n","            nn.ReLU()\n","        )\n","        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n","        self.fc_lstm = nn.Linear(hidden_size, 128)\n","        self.fc = nn.Linear(128*2, num_classes)\n","\n","    def forward(self, x):\n","        #cnn takes input of shape (batch_size, channels, seq_len)\n","        x_cnn = x.permute(0, 2, 1)\n","        out_cnn = self.cnn(x_cnn)\n","        # lstm takes input of shape (batch_size, seq_len, input_size)\n","        out_lstm, _ = self.lstm(x)\n","        out_lstm = self.fc_lstm(out_lstm[:, -1, :])\n","        out = torch.cat([out_cnn, out_lstm], dim=1)\n","        out = self.fc(out)\n","        return out\n","\n","\n","class Custom_df(Dataset):\n","    def __init__(self, x, y):\n","        self.x = x\n","        self.y = y\n","\n","    def __getitem__(self, index): #this method allows to retrieve a specific sample from the dataset based on its index (the index is passed to this method)\n","        input = torch.tensor(np.array(self.x[index]), dtype=torch.float32) #convert np array into a tensor\n","        target = torch.tensor(np.array(self.y[index]), dtype=torch.float32)\n","        return input, target\n","\n","    def __len__(self):\n","        return len(self.x)\n","\n","\n","class DeviceDataLoader (): #receive a dataloader and move to the correct device\n","  def __init__(self, dl : DataLoader, device):\n","    self.dl = dl\n","    self.device = device\n","\n","  def __iter__(self):\n","    for batch in iter(self.dl): #iter will reset the iterator every time the __iter__ mathod gets called\n","      yield ADE.to_device(batch, device)\n","\n","  def __len__(self):\n","    return len(self.dl)\n","\n","\n","\n","class ADE ():\n","    def __init__(self, trained_models: list, meta_model, train_data: pd.DataFrame, test_data: pd.DataFrame, window_size : int = 10, error_window = 10, k=3, temperature=1, stacking_preds = None):\n","        self.trained_models = trained_models\n","        self.meta_model = self.get_meta_model(meta_model) #accept a string as an input\n","        self.train_data = pd.DataFrame(train_data)\n","        self.test_data = pd.DataFrame(test_data)\n","        self.MinMaxscaler = MinMaxScaler(feature_range=(-1, 1))\n","        self.Robustscaler = RobustScaler()\n","        # self.error_scaler = MinMaxScaler(feature_range=(-1, 1))\n","        self.window_size = window_size\n","        self.error_window = error_window\n","        self.k = k\n","        self.temperature = temperature\n","        self.test_predictions = []\n","        self.test_error_predictions = []\n","        self.device = self.get_device()\n","        self.lock = threading.Lock()\n","        self.ensemble_preds = None\n","        self.tscv = OverlappingTimeSeriesSplit(n_splits=5, window_size=window_size)\n","        self.oof_predictions = []\n","        self.first_fold_idx = None\n","        self.target_scaler = RobustScaler()\n","        self.stacking_preds = stacking_preds\n","\n","\n","        #if there are deep learning models\n","        if any(isinstance(model, torch.nn.Module) for model in trained_models):\n","            self.train_loader, self.test_loader = self.data_preprocessing_deep_learning(self.train_data, self.test_data, self.MinMaxscaler, self.window_size)\n","\n","        #if there are machine learning models\n","        if any(not isinstance(model, torch.nn.Module) for model in trained_models):\n","            self.X_train, self.y_train, self.X_test, self.y_test = self.data_preprocessing_machine_learning(self.train_data, self.test_data, self.Robustscaler, self.window_size)\n","\n","\n","    def feature_engineer(self, oof_predictions, real_data):\n","        # print(\"feature engineering\")\n","        df = pd.DataFrame(index = real_data.index)\n","        oof_predictions = pd.DataFrame(oof_predictions, index = real_data.index)\n","\n","        #compute errors\n","        ae_errors = np.abs(real_data.iloc[:,0] - oof_predictions.iloc[:,0])\n","\n","        # print(\"ae errors\")\n","        # print(ae_errors.head())\n","\n","        #compute rolling MAE\n","        rolling_mae = np.zeros(len(oof_predictions))  # Assuming oof_predictions is 1D\n","\n","        for i in range(len(oof_predictions)):\n","            start = max(0, i - self.window_size + 1)\n","            errors = ae_errors.iloc[start:i+1]\n","\n","            # Calculate MAE for the current window\n","            rolling_mae[i] = np.mean(errors)\n","\n","\n","        #target\n","        # df['mae'] = rolling_mae\n","\n","\n","        df['mae'] = ae_errors.rolling(window=self.window_size).mean().fillna(0)\n","\n","\n","\n","        # decay_factor = 0.90  # Higher weight for recent errors\n","        # weights = np.array([decay_factor**(self.window_size - i) for i in range(self.window_size)])\n","\n","        # # Enhanced Weighted MAE Calculation\n","        # weighted_errors = []\n","        # for i in range(len(oof_predictions)):\n","        #     start = max(0, i - self.window_size + 1)\n","        #     window_errors = ae_errors.iloc[start:i+1]\n","        #     if len(window_errors) < self.window_size:\n","        #         adjusted_weights = weights[-len(window_errors):] / np.sum(weights[-len(window_errors):])\n","        #     else:\n","        #         adjusted_weights = weights / np.sum(weights)\n","        #     weighted_errors.append(np.dot(window_errors, adjusted_weights))\n","\n","        # df['mae'] = pd.Series(weighted_errors, index=df.index)\n","\n","\n","\n","\n","\n","        #covariates\n","        for lag in range(1, 3):\n","            df[f'prediction_lag_{lag}'] = oof_predictions.shift(lag).fillna(0)\n","            df[f'ae_error_lag_{lag}'] = ae_errors.shift(lag).fillna(0)\n","            df[f\"actual_lag_{lag}\"] = real_data.shift(lag).fillna(0)\n","\n","        for lag in range(1, 5):\n","            df[f\"mae_lag_{lag}\"] = df[\"mae\"].shift(lag).fillna(0)\n","\n","        df['prediction'] = oof_predictions\n","\n","\n","        #rolling statistics\n","        df['rolling_mean_ae_3d'] = ae_errors.rolling(3).mean().shift(1).fillna(0)\n","        df['rolling_std_ae_10d'] = ae_errors.rolling(10).std().shift(1).fillna(0)\n","\n","        #trend\n","        df['trend_mae'] = df['mae'].shift(1).fillna(0) - df['mae'].shift(2).fillna(0)\n","        df['trend_ae'] = ae_errors.shift(1).fillna(0) - ae_errors.shift(2).fillna(0)\n","\n","        #interaction term\n","        df['interation_prediction_actual'] = df['prediction'] * df['actual_lag_1']\n","        df['mae_actual_ratio'] = df['mae'].shift(1).fillna(0) * df['prediction']\n","\n","\n","        #new metrics\n","        df['pred_pct_change'] = oof_predictions.iloc[:,0].pct_change().abs().fillna(0)\n","\n","\n","        df['squared_error'] = np.abs(ae_errors**2).shift(1).fillna(0)\n","\n","        signed_error = real_data.iloc[:,0] - oof_predictions.iloc[:,0]\n","        df['overprediction_flag_lag1'] = ((signed_error.shift(1) > 0).astype(int).fillna(0))\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","        #exponential moving average\n","\n","\n","\n","\n","        # df['error'] = ae_errors\n","\n","\n","\n","        #visualize\n","        # print(df)\n","\n","        return df\n","\n","\n","\n","\n","\n","\n","    def meta_model_preprocessing(self, oof_predictions, real_data, scaler, is_train = False):\n","        df = self.feature_engineer(oof_predictions, real_data)\n","\n","        #save df\n","        if is_train == True:\n","            df.to_csv(\"train_df.csv\")\n","        else:\n","            df.to_csv(\"test_df.csv\")\n","\n","        if is_train == True:\n","            self.target_scaler.fit(df[['mae']])\n","\n","        #scale\n","        df_scaled = pd.DataFrame(scaler.fit_transform(df), index=df.index, columns=df.columns)\n","\n","        X , y = df_scaled.drop('mae', axis=1) , df_scaled['mae']\n","\n","        return X, y\n","\n","\n","\n","\n","\n","    def get_meta_model(self, meta_model): #get pre trained models\n","\n","        try:\n","            if meta_model == \"linear booster\":\n","\n","                # best_params = {\"n_estimators\": 1,  \"max_depth\": 10,  \"min_samples_split\" : 10, \"min_samples_leaf\" : 0.2 }\n","                # base_estimator = Ridge(alpha = 0.5)\n","\n","                best_params = {\"n_estimators\": 1,  \"max_depth\": 5,  \"min_samples_split\" : 5, \"min_samples_leaf\" : 0.2 }\n","                base_estimator = LinearRegression()\n","\n","                return LinearBoostRegressor(base_estimator=base_estimator, **best_params)\n","\n","\n","            elif meta_model == \"linear forest\":\n","                best_params = {\"n_estimators\": 500,  \"max_depth\": 5,  \"min_samples_split\" : 5, \"min_samples_leaf\" : 0.3 , 'max_features' : \"log2\"}\n","                base_estimator = LinearRegression()\n","\n","                return LinearForestRegressor(base_estimator = base_estimator, **best_params)\n","\n","\n","        except Exception as e:\n","            raise ValueError(f\"Unsupported base model type: {type(meta_model)}\")\n","\n","\n","\n","    def sliding_windows(self, data, seq_length): #helper function to genereate sliding windows\n","        X, y = [], []\n","\n","        for i in range(len(data) - seq_length ):\n","            _x = data.iloc[i:i+seq_length]\n","            _y = data.iloc[i + seq_length]\n","            X.append(_x)\n","            y.append(_y)\n","\n","        return np.array(X), np.array(y)\n","\n","\n","    def get_device(self):\n","        if torch.cuda.is_available():\n","            device = 'cuda'\n","        else:\n","            device = 'cpu'\n","\n","        return device\n","\n","\n","    @staticmethod\n","    def to_device(data, device):\n","        if isinstance(data, (list, tuple)):\n","            return [to_device(x, device) for x in data]\n","        elif isinstance(data, torch.Tensor):  # Only move tensors to the device\n","            return data.to(device, non_blocking=True)\n","        else:\n","            return data  # For non-tensor types (e.g., strings), return as is\n","\n","\n","    @torch.no_grad()\n","    def inference(self, model, data_loader):\n","        model.eval()  # Set the model to evaluation mode\n","        predictions = []\n","        actuals = []\n","\n","        for batch in data_loader:\n","            input, output = batch  # Unpack your batch into input and output\n","            outputs = model(input) # Perform the forward pass\n","\n","            # Move outputs and actuals back to CPU and append to lists\n","            predictions.append(outputs.cpu())\n","            actuals.append(output.cpu())\n","\n","        # Concatenate all predictions and actuals into single tensors\n","        predictions = torch.cat(predictions, dim=0)\n","        actuals = torch.cat(actuals, dim=0)\n","\n","        return predictions, actuals\n","\n","\n","    def data_preprocessing_deep_learning (self, train_data, test_data, scaler, time_window):\n","        #scale data\n","        train_data_scaled = pd.DataFrame(scaler.fit_transform(train_data))\n","        test_data_scaled = pd.DataFrame(scaler.transform(test_data))\n","\n","        #create sliding windows\n","        X_train, y_train = self.sliding_windows(train_data_scaled, time_window)\n","        X_test, y_test = self.sliding_windows(test_data_scaled, time_window)\n","\n","        #move data to tensors\n","        train_df = Custom_df(X_train, y_train)\n","        test_df = Custom_df(X_test, y_test)\n","\n","        #create dataloaders (to perform training/inference in batch)\n","        batch_size = 128\n","        num_workers = 2\n","        pin_memory = True if self.device == \"cuda\" else False\n","        train_loader = DataLoader(train_df, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory, persistent_workers=True)\n","        test_loader = DataLoader(test_df, batch_size=batch_size*2, shuffle=False, num_workers=num_workers, pin_memory=pin_memory, persistent_workers=True)\n","\n","        #move to device\n","        train_loader = DeviceDataLoader(train_loader, self.device)\n","        test_loader = DeviceDataLoader(test_loader, self.device)\n","\n","        return train_loader, test_loader\n","\n","\n","    def data_preprocessing_machine_learning(self, train_data, test_data, scaler, time_window):\n","        #convert data to numpy arrays\n","        if isinstance(train_data, torch.Tensor):\n","            train_data = train_data.numpy()\n","        if isinstance(test_data, torch.Tensor):\n","            test_data = test_data.numpy()\n","\n","        #scale data\n","\n","        if isinstance(train_data, pd.Series):\n","            train_data_scaled = pd.DataFrame(scaler.fit_transform(train_data.values.reshape(-1,1)))\n","            test_data_scaled = pd.DataFrame(scaler.transform(test_data.values.reshape(-1,1)))\n","\n","        else:\n","            train_data_scaled = pd.DataFrame(scaler.fit_transform(train_data))\n","            test_data_scaled = pd.DataFrame(scaler.transform(test_data))\n","\n","        #create sliding windows\n","        X_train, y_train = self.sliding_windows(train_data_scaled, time_window)\n","        X_test, y_test = self.sliding_windows(test_data_scaled, time_window)\n","\n","        # Flatten the sliding windows\n","        X_train = X_train.reshape(X_train.shape[0], -1)\n","        X_test = X_test.reshape(X_test.shape[0], -1)\n","\n","        return X_train, y_train, X_test, y_test\n","\n","\n","    def k_fold_model_prediction(self, model):\n","        try:\n","            if isinstance(model, LinearBoostRegressor): #Linear Boost model predictions\n","\n","                #compute k fold out of sample predictions\n","                oof_predictions = np.zeros(len(self.train_data))\n","\n","                with self.lock:\n","                    for fold, (train_idx, test_idx) in enumerate(self.tscv.split(range(len(self.train_data)))):\n","\n","                        #save first fold (unused)\n","                        if fold == 0:\n","                            self.first_fold_idx = test_idx[self.window_size]\n","\n","\n","                        #data preprocessing\n","                        train_df, test_df = self.train_data.iloc[train_idx], self.train_data.iloc[test_idx]\n","\n","                        #X,y\n","                        scaler = RobustScaler()\n","                        X_train, y_train, X_test, y_test = self.data_preprocessing_machine_learning(train_df, test_df, scaler, self.window_size)\n","\n","                        #predict\n","                        test_prediction = model.predict(X_test)\n","\n","                        #inverse scale\n","                        test_prediction = scaler.inverse_transform(test_prediction.reshape(-1, 1))\n","\n","                        # Store predictions in the correct indices\n","                        oof_predictions[test_idx[self.window_size:]] = test_prediction.flatten()\n","\n","\n","                    #visualize predictions\n","                    plt.figure(figsize=(12, 6))\n","                    sns.lineplot(x= self.train_data[self.window_size:].index, y = self.train_data[self.window_size:].squeeze(), label=\"Train Data\")\n","                    plt.title('Out of sample XGBoost Predictions on Train set')\n","                    sns.lineplot(x= self.train_data[self.first_fold_idx:].index, y=oof_predictions[self.first_fold_idx:].squeeze(), label=\"Out of sample XGBoost predictions on Train set\")\n","                    plt.xlabel('Date')\n","                    plt.ylabel('Close price')\n","                    plt.xticks(rotation=45)  # Rotate x-axis labels\n","                    plt.tight_layout() # Automatically adjusts the layout\n","                    plt.legend()\n","                    plt.show()\n","\n","                    r2 = r2_score(self.train_data[self.first_fold_idx:].values, oof_predictions[self.first_fold_idx:])\n","                    print(f\"R2 score: {r2}\")\n","\n","                # return (oof_predictions[self.first_fold_idx:], train_volatility)\n","                return (oof_predictions[self.first_fold_idx:])\n","\n","\n","            elif isinstance(model, torch.nn.Module):\n","\n","\n","                #compute k fold out of sample predictions\n","                oof_predictions = np.zeros(len(self.train_data))\n","\n","                with self.lock:\n","                    for fold, (train_idx, test_idx) in enumerate(self.tscv.split(range(len(self.train_data)))):\n","\n","                        #save first fold (unused)\n","                        if fold == 0:\n","                            self.first_fold_idx = test_idx[self.window_size]\n","\n","                        #data preprocessing\n","                        train_df, test_df = self.train_data.iloc[train_idx], self.train_data.iloc[test_idx]\n","\n","                        #X,y\n","                        scaler = MinMaxScaler(feature_range=(-1, 1))\n","                        train_loader, test_loader = self.data_preprocessing_deep_learning(train_df, test_df, scaler, self.window_size)\n","\n","                        #predict\n","                        test_prediction, test_actual = self.inference(model, test_loader)\n","\n","                        #inverse scale\n","                        test_prediction = scaler.inverse_transform(test_prediction)\n","\n","                        # Store predictions in the correct indices\n","                        oof_predictions[test_idx[self.window_size:]] = test_prediction.flatten()\n","\n","\n","\n","                    #visualize predictions\n","                    plt.figure(figsize=(12, 6))\n","                    sns.lineplot(x= self.train_data[self.window_size:].index, y = self.train_data[self.window_size:].squeeze(), label=\"Train Data\")\n","                    sns.lineplot(x= self.train_data[self.first_fold_idx:].index, y=oof_predictions[self.first_fold_idx:].squeeze(), label=f\"Out of sample {model.__class__.__name__} predictions on Train set\")\n","                    plt.title(f\"Out of sample {model.__class__.__name__} predictions on Train set\")\n","                    plt.xlabel('Date')\n","                    plt.ylabel('Close price')\n","                    plt.xticks(rotation=45)  # Rotate x-axis labels\n","                    plt.tight_layout() # Automatically adjusts the layout\n","                    plt.legend()\n","                    plt.show()\n","\n","                    r2 = r2_score(self.train_data[self.first_fold_idx:].values, oof_predictions[self.first_fold_idx:])\n","                    print(f\"R2 score: {r2}\")\n","\n","                    # Explicit cleanup\n","                    del train_loader, test_loader\n","                    torch.cuda.empty_cache()\n","\n","                # return (oof_predictions[self.first_fold_idx:], train_volatility)\n","                return (oof_predictions[self.first_fold_idx:])\n","\n","            else:\n","                raise ValueError(f\"Unsupported base model type: {type(model)}\")\n","\n","\n","        except Exception as e:\n","            print(f\"Error processing model {model}: {e}\")\n","            raise e\n","\n","\n","    def model_prediction(self, model, oof_predictions):\n","        try:\n","            if isinstance(model, LinearBoostRegressor): #Linear Boost model predictions\n","\n","\n","                #data preprocessing\n","                with self.lock:\n","                    scaler = RobustScaler()\n","                    X_train, y_train = self.meta_model_preprocessing(oof_predictions, self.train_data[self.first_fold_idx:], scaler, is_train = True)\n","\n","                    #train meta model\n","                    self.meta_model.fit(X_train, y_train)\n","                    print('meta model trained')\n","                    MAE_predictions = self.meta_model.predict(X_train)\n","\n","                    #unscale\n","                    MAE_predictions = self.target_scaler.inverse_transform(MAE_predictions.reshape(-1, 1))\n","                    y_train = self.target_scaler.inverse_transform(y_train.to_numpy().reshape(-1, 1))\n","\n","\n","\n","                    #visualize predictions\n","                    plt.figure(figsize=(12, 6))\n","                    sns.lineplot(x= self.train_data[self.first_fold_idx :].index, y = y_train.ravel(), label=\"train MAE\")\n","                    sns.lineplot(x= self.train_data[self.first_fold_idx :].index, y=MAE_predictions.squeeze(), label=\"MAE predicted\")\n","                    plt.title('MAE predictions on Train set Linear Boost')\n","                    plt.xlabel('Date')\n","                    plt.ylabel('MAE')\n","                    plt.xticks(rotation=45)  # Rotate x-axis labels\n","                    plt.tight_layout() # Automatically adjusts the layout\n","                    plt.legend()\n","                    plt.show()\n","\n","                    #compute r2\n","                    r2 = r2_score(y_train, MAE_predictions)\n","                    print(f\"R2 score: {r2}\")\n","\n","                    #test predictions\n","                    test_prediction = model.predict(self.X_test)\n","\n","                    #unscale\n","                    test_prediction = self.Robustscaler.inverse_transform(test_prediction.reshape(-1, 1))\n","\n","                    #data preprocessing\n","                    X_test, y_test = self.meta_model_preprocessing(test_prediction, self.test_data[self.window_size:], scaler)\n","\n","                    #meta model predict\n","                    MAE_predictions = self.meta_model.predict(X_test)\n","\n","                    #unscale predicions\n","                    MAE_predictions = self.target_scaler.inverse_transform(MAE_predictions.reshape(-1, 1))\n","                    y_test = self.target_scaler.inverse_transform(y_test.to_numpy().reshape(-1, 1))\n","\n","                    #visualize predictions\n","                    plt.figure(figsize=(12, 6))\n","                    sns.lineplot(x= self.test_data[self.window_size :].index, y = y_test.ravel(), label=\"Test MAE\")\n","                    sns.lineplot(x= self.test_data[self.window_size:].index, y=MAE_predictions.squeeze(), label=\"MAE predicted\")\n","                    plt.title('MAE predictions on Test set Linear Boost')\n","                    plt.xlabel('Date')\n","                    plt.ylabel('MAE')\n","                    plt.xticks(rotation=45)  # Rotate x-axis labels\n","                    plt.tight_layout() # Automatically adjusts the layout\n","                    plt.legend()\n","                    plt.show()\n","\n","                    #compute r2\n","                    r2 = r2_score(y_test, MAE_predictions)\n","                    print(f\"R2 score: {r2}\")\n","\n","\n","                return (torch.tensor(test_prediction, dtype=torch.float32, device = self.device).detach().squeeze() , torch.tensor(MAE_predictions, dtype=torch.float32).detach())\n","                # return (torch.tensor(test_prediction, dtype=torch.float32, device = self.device).detach().squeeze() , torch.tensor(y_test, dtype=torch.float32).detach())\n","\n","            elif isinstance(model, torch.nn.Module):\n","\n","                #data preprocessing\n","                scaler = RobustScaler()\n","                with self.lock:\n","                    X_train, y_train = self.meta_model_preprocessing(oof_predictions, self.train_data[self.first_fold_idx:], scaler, is_train = True)\n","\n","\n","                    #train meta model\n","                    self.meta_model.fit(X_train, y_train)\n","                    print('meta model trained')\n","                    MAE_predictions = self.meta_model.predict(X_train)\n","\n","                    #unscale\n","                    MAE_predictions = self.target_scaler.inverse_transform(MAE_predictions.reshape(-1, 1))\n","                    y_train = self.target_scaler.inverse_transform(y_train.to_numpy().reshape(-1, 1))\n","\n","\n","                    #visualize predictions\n","                    plt.figure(figsize=(12, 6))\n","                    sns.lineplot(x= self.train_data[self.first_fold_idx :].index, y = y_train.ravel(), label=\"train MAE\")\n","                    sns.lineplot(x= self.train_data[self.first_fold_idx :].index, y=MAE_predictions.squeeze(), label=\"MAE predicted\")\n","                    plt.title(f'MAE predictions on Train set {model.__class__.__name__}')\n","                    plt.xlabel('Date')\n","                    plt.ylabel('MAE')\n","                    plt.xticks(rotation=45)  # Rotate x-axis labels\n","                    plt.tight_layout() # Automatically adjusts the layout\n","                    plt.legend()\n","                    plt.show()\n","\n","                    #compute r2\n","                    r2 = r2_score(y_train, MAE_predictions)\n","                    print(f\"R2 score: {r2}\")\n","\n","\n","                    #test predictions\n","                    test_prediction, test_actual = self.inference(model, self.test_loader)\n","\n","                    #unscale\n","                    test_prediction = self.MinMaxscaler.inverse_transform(test_prediction.reshape(-1, 1))\n","\n","\n","                    #data preprocessing\n","                    X_test, y_test = self.meta_model_preprocessing(test_prediction, self.test_data[self.window_size:], scaler)\n","\n","\n","                    #meta model predict\n","                    MAE_predictions = self.meta_model.predict(X_test)\n","\n","                    #unscale predicions\n","                    MAE_predictions = self.target_scaler.inverse_transform(MAE_predictions.reshape(-1, 1))\n","                    y_test = self.target_scaler.inverse_transform(y_test.to_numpy().reshape(-1, 1))\n","\n","                    #visualize predictions\n","                    plt.figure(figsize=(12, 6))\n","                    sns.lineplot(x= self.test_data[self.window_size :].index, y = y_test.ravel(), label=\"Test MAE\")\n","                    sns.lineplot(x= self.test_data[self.window_size:].index, y=MAE_predictions.squeeze(), label=\"MAE predicted\")\n","                    plt.title(f'MAE predictions on Test set {model.__class__.__name__}')\n","                    plt.xlabel('Date')\n","                    plt.ylabel('MAE')\n","                    plt.xticks(rotation=45)  # Rotate x-axis labels\n","                    plt.tight_layout() # Automatically adjusts the layout\n","                    plt.legend()\n","                    plt.show()\n","\n","                    #compute r2\n","                    r2 = r2_score(y_test, MAE_predictions)\n","                    print(f\"R2 score: {r2}\")\n","\n","\n","\n","\n","                return (torch.tensor(test_prediction, dtype=torch.float32, device=self.device).detach().squeeze() , torch.tensor(MAE_predictions, dtype=torch.float32).detach())\n","                # return (torch.tensor(test_prediction, dtype=torch.float32, device=self.device).detach().squeeze() , torch.tensor(y_test, dtype=torch.float32).detach())\n","\n","            else:\n","                raise ValueError(f\"Unsupported base model type: {type(model)}\")\n","\n","\n","\n","\n","        except Exception as e:\n","            print(f\"Error processing model {model}: {e}\")\n","            raise e\n","\n","\n","    def predict(self):\n","        #k fold train predictions\n","        print(\"out of sample predictions\")\n","        with ThreadPoolExecutor() as executor: #it creates a pool of worker threads (the with statement ensures that the pool of threads is cleaned up automatically after the execution)\n","            results = list(executor.map(self.k_fold_model_prediction, self.trained_models)) #the task each thread will execute\n","\n","\n","        # Combine oof predictions into a list\n","        self.oof_predictions = [item for item in results]\n","\n","\n","        #test predictions\n","        with ThreadPoolExecutor() as executor: #it creates a pool of worker threads (the with statement ensures that the pool of threads is cleaned up automatically after the execution)\n","            results = list(executor.map(self.model_prediction, self.trained_models, self.oof_predictions)) #the task each thread will execute\n","\n","\n","        # Combine predictions into a single tensor\n","        self.test_predictions = torch.stack([item[0] for item in results], dim=1).squeeze()\n","        self.test_error_predictions = torch.stack([item[1] for item in results], dim=1).squeeze()\n","\n","        print(\"test predictions\")\n","        print(self.test_predictions)\n","\n","\n","\n","        print(\"test error predictions\")\n","        print(self.test_error_predictions)\n","\n","\n","        #filter the 3 best errors\n","        best_kerrors, indices = torch.topk(self.test_error_predictions, k=self.k, largest=False, sorted=False)\n","        print(\"best k errors\")\n","        print(best_kerrors)\n","\n","\n","        #normalize the errors\n","        std = best_kerrors.std(dim=1, keepdim=True) + 1e-8 #for numerical stability (to avoid division by zero)\n","        print(\"std\")\n","        print(std[:10])\n","        mean = best_kerrors.mean(dim=1, keepdim=True)\n","        print(\"mean\")\n","        print(mean[:10])\n","        normalized_errors = (self.test_error_predictions - mean) / std\n","        print(\"unmasked normalized errors\")\n","        print(normalized_errors)\n","\n","\n","        #apply mask\n","        mask = torch.zeros_like(self.test_error_predictions, dtype=torch.bool)\n","        mask.scatter_(dim=1, index=indices, value=True)\n","        normalized_errors[~mask] = float('inf')\n","        print(\"normalized errors\")\n","        print(normalized_errors)\n","\n","\n","        #apply softmax\n","        weights_df = F.softmax(-normalized_errors, dim=1)\n","        print(\"weights\")\n","        print(weights_df)\n","\n","        weights_df = F.softmax(-normalized_errors * self.temperature, dim=1)\n","        print(\"temperature weights\")\n","        print(weights_df)\n","\n","        #combine predictions\n","        self.ensemble_preds = self.test_predictions * weights_df\n","        self.ensemble_preds = self.ensemble_preds.sum(dim=1)\n","\n","\n","        return self.ensemble_preds"]},{"cell_type":"markdown","source":["Linear booster meta model"],"metadata":{"id":"Ch11Gl_pfo85"}},{"cell_type":"code","source":["#using the class\n","train_data = {}\n","test_data = {}\n","\n","trained_models = [Linear_booster_model, CNN_model, LSTM_model, LSTM_CNN_model, parallelCNNLSTM_model]\n","\n","for name , _ in datasets.items():\n","\n","    # if name == 'sp500':\n","    #     continue\n","\n","    # if name == 'eur_usd':\n","    #     continue\n","\n","    print(f\"ensembling predictions on {name} dataset:\")\n","    meta_model = \"linear booster\"\n","    # meta_model = \"linear forest\"\n","\n","    train_data[name] = val_residuals[name]\n","    train_data[name].index = pd.to_datetime(val_residuals[name].index)\n","\n","    test_data[name] = test_residuals[name]\n","    test_data[name].index = pd.to_datetime(test_residuals[name].index)\n","    window_size = 10\n","    error_window = 10\n","\n","    if name == 'eur_usd':\n","        # k=4\n","        # temperature = 1 #to control weight sharpness\n","\n","        k=3\n","        temperature = 0.65 #to control weight sharpness\n","    elif name == 'sp500':\n","        temperature = 0.05\n","        k=3 #number of models to make the ensemble at each time step\n","\n","\n","\n","    ade = ADE([trained_model[name] for trained_model in trained_models], meta_model, train_data[name], test_data[name], window_size, error_window, k, temperature) #k represent the number of model to select from the batch of models\n","    final_preds = ade.predict()\n","\n","\n","    #visualize\n","    ensemble_preds = pd.DataFrame(final_preds.detach().numpy(), index = test_data[name][window_size:].index)\n","    # ensemble_preds.index = pd.to_datetime(test_data[name][window_size*2:].index)\n","\n","    plt.figure(figsize=(12, 6))\n","    plt.title(f\"ADE ensemble predictions {name}\")\n","    sns.lineplot(x = ensemble_preds.index, y = ensemble_preds.squeeze(), label = \"ADE\", color = \"red\")\n","    sns.lineplot(x = test_data[name][window_size*2:].index, y = test_data[name][window_size*2:].squeeze(), label = \"test set\", color = \"blue\")\n","    # sns.lineplot(x = test_data[name][window_size:].index, y = test_data[name][window_size:].squeeze(), label = \"test set\", color = \"blue\")\n","    plt.ylabel('Close price residuals')\n","    plt.xlabel('Date')\n","    plt.xticks(rotation=45)  # Rotate x-axis labels\n","    plt.tight_layout() # Automatically adjusts the layout\n","    plt.legend()\n","    plt.show()\n","\n","\n","    #compute metrics\n","    # mse = mean_squared_error(test_data[name][window_size*2:], ensemble_preds)\n","    # rmse = np.sqrt(mse)\n","    # mae = mean_absolute_error(test_data[name][window_size*2:], ensemble_preds)\n","    # r2 = r2_score(test_data[name][window_size*2:], ensemble_preds)\n","    # mape = mean_absolute_percentage_error(test_data[name][window_size*2:], ensemble_preds)\n","    # smape_value = smape(test_data[name][window_size*2:], ensemble_preds)\n","\n","    mse = mean_squared_error(test_data[name][window_size:], ensemble_preds)\n","    rmse = np.sqrt(mse)\n","    mae = mean_absolute_error(test_data[name][window_size:], ensemble_preds)\n","    r2 = r2_score(test_data[name][window_size:], ensemble_preds)\n","    mape = mean_absolute_percentage_error(test_data[name][window_size:], ensemble_preds)\n","    smape_value = smape(test_data[name][window_size:], ensemble_preds)\n","\n","    print(f\"MSE: {mse}\")\n","    print(f\"RMSE: {rmse}\")\n","    print(f\"MAE: {mae}\")\n","    print(f\"r2:  {r2}\")\n","    print(f\"MAPE: {mape}\")\n","    print(f\"SMAPE: {smape_value}\")\n","\n","\n","    #combine ensemble predictions with ARIMA model\n","    final_preds = SARIMA_test_predictions[name][window_size:] + ensemble_preds.iloc[:,0]\n","\n","    #visualize\n","    plt.figure(figsize=(12, 6))\n","    plt.title(f\"SARIMA + ensemble predictions {name}\")\n","    sns.lineplot(x = final_preds.index, y = final_preds.squeeze(), label = \"SARIMA + ensemble predictions\", color = \"red\")\n","    sns.lineplot(x = Close_price[name].loc[final_preds.index].index, y = Close_price[name].loc[final_preds.index].squeeze(), label = \"test set\", color = \"blue\")\n","    plt.ylabel('Close price')\n","    plt.xlabel('Date')\n","    plt.xticks(rotation=45)  # Rotate x-axis labels\n","    plt.tight_layout() # Automatically adjusts the layout\n","    plt.legend()\n","    plt.show()\n","\n","\n","    #compute metrics\n","    mse = mean_squared_error(Close_price[name].loc[final_preds.index], final_preds)\n","    rmse = np.sqrt(mse)\n","    mae = mean_absolute_error(Close_price[name].loc[final_preds.index], final_preds)\n","    r2 = r2_score(Close_price[name].loc[final_preds.index], final_preds)\n","    mape = mean_absolute_percentage_error(Close_price[name].loc[final_preds.index], final_preds)\n","    smape_value = smape(Close_price[name].loc[final_preds.index], final_preds)\n","\n","    print(f\"MSE: {mse}\")\n","    print(f\"RMSE: {rmse}\")\n","    print(f\"MAE: {mae}\")\n","    print(f\"r2:  {r2}\")\n","    print(f\"MAPE: {mape}\")\n","    print(f\"SMAPE: {smape_value}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1v6xdh6RqZH8mWl1ceLq-n6NdWBiPBpxJ"},"id":"WFmv3ZiVf61r","outputId":"c8a71b50-fad4-4e52-83ea-b4a0549baa34","executionInfo":{"status":"ok","timestamp":1740611341648,"user_tz":-60,"elapsed":49087,"user":{"displayName":"Andrea Damiano","userId":"07680645090793871056"}}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["#Static ensemble techniques"],"metadata":{"id":"pRG2Cj7KFRc_"}},{"cell_type":"markdown","source":["Stacking"],"metadata":{"id":"pkHHKS8uPxui"}},{"cell_type":"code","source":["#import libraries\n","import pandas as pd\n","import numpy as np\n","from google.colab import drive\n","import datetime\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","import statsmodels.api as sm\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, Subset\n","from joblib import Parallel, delayed, dump, load\n","import xgboost as xgb\n","import time\n","import yfinance as yf\n","import torch.nn.functional as F\n","from concurrent.futures import ThreadPoolExecutor\n","import threading\n","from sklearn.linear_model import LogisticRegression\n","# from sklearn.model_selection import TimeSeriesSplit\n","\n","#suppress warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","\n","class OverlappingTimeSeriesSplit():\n","    def __init__(self, n_splits=5, window_size=10):\n","        self.n_splits = n_splits\n","        self.window_size = window_size\n","\n","    def split(self, X):\n","        n_samples = len(X)\n","        fold_size = n_samples // (self.n_splits + 1)  # Ensure proper splitting\n","\n","        for i in range(1, self.n_splits + 1):  # Start from 1 to avoid empty train set\n","            train_end = i * fold_size\n","            train_idx = range(train_end)  # Train set includes all data before validation\n","\n","            val_start = max(0, train_end - self.window_size)\n","            val_end = train_end + fold_size\n","            val_idx = range(val_start, min(val_end, n_samples))  # Ensure within bounds\n","\n","            yield list(train_idx), list(val_idx)\n","\n","def smape(y_true, y_pred):\n","    y_true = np.array(y_true)\n","    y_pred = np.array(y_pred)\n","\n","    numerator = np.abs(y_pred - y_true)\n","    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n","    smape_value = np.mean(numerator / denominator)  # Renamed variable to avoid conflict\n","    return smape_value\n","\n","#deep learning meta model architecture\n","class LSTM(nn.Module):\n","\n","    def __init__(self, num_classes, input_size, hidden_size=512, num_layers=3, dropout=0.3):\n","        super().__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.num_classes = num_classes\n","\n","        # LSTM layers\n","        self.lstm = nn.LSTM(\n","            input_size=input_size,\n","            hidden_size=hidden_size,\n","            num_layers=num_layers,\n","            batch_first=True,\n","            dropout=dropout\n","        )\n","\n","        # Fully connected layers\n","        self.fc1 = nn.Linear(hidden_size, 256)\n","        self.fc2 = nn.Linear(256, 128)\n","        self.fc3 = nn.Linear(128, self.num_classes)\n","\n","        # Activation and dropout\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","    def forward(self, x):\n","        # LSTM output\n","        lstm_out, _ = self.lstm(x)\n","\n","        # Pass only the last output\n","        lstm_out = lstm_out[:, -1, :]\n","\n","        # Fully connected layers with ReLU and Dropout\n","        x = self.relu(self.fc1(lstm_out))\n","        x = self.dropout(x)\n","        x = self.relu(self.fc2(x))\n","        x = self.dropout(x)\n","        x = self.fc3(x)  # Final prediction\n","        return x\n","\n","\n","#deep learning meta model architecture\n","class ParallelCNN_LSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n","        super().__init__()\n","        self.cnn = nn.Sequential(\n","            nn.Conv1d(in_channels=input_size, out_channels=64, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool1d(kernel_size=2, stride=2),\n","            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool1d(kernel_size=2, stride=2),\n","            nn.Flatten(),\n","            nn.LazyLinear(out_features=128), #linear layer that automatically infer the input size\n","            nn.ReLU()\n","        )\n","        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n","        self.fc_lstm = nn.Linear(hidden_size, 128)\n","        self.fc = nn.Linear(128*2, num_classes)\n","\n","    def forward(self, x):\n","        #cnn takes input of shape (batch_size, channels, seq_len)\n","        x_cnn = x.permute(0, 2, 1)\n","        out_cnn = self.cnn(x_cnn)\n","        # lstm takes input of shape (batch_size, seq_len, input_size)\n","        out_lstm, _ = self.lstm(x)\n","        out_lstm = self.fc_lstm(out_lstm[:, -1, :])\n","        out = torch.cat([out_cnn, out_lstm], dim=1)\n","        out = self.fc(out)\n","        return out\n","\n","\n","class Custom_df(Dataset):\n","    def __init__(self, x, y):\n","        self.x = x\n","        self.y = y\n","\n","    def __getitem__(self, index): #this method allows to retrieve a specific sample from the dataset based on its index (the index is passed to this method)\n","        input = torch.tensor(np.array(self.x[index]), dtype=torch.float32) #convert np array into a tensor\n","        target = torch.tensor(np.array(self.y[index]), dtype=torch.float32)\n","        return input, target\n","\n","    def __len__(self):\n","        return len(self.x)\n","\n","\n","class DeviceDataLoader (): #receive a dataloader and move to the correct device\n","  def __init__(self, dl : DataLoader, device):\n","    self.dl = dl\n","    self.device = device\n","\n","  def __iter__(self):\n","    for batch in iter(self.dl): #iter will reset the iterator every time the __iter__ mathod gets called\n","      yield Stacking.to_device(batch, device)\n","\n","  def __len__(self):\n","    return len(self.dl)\n","\n","\n","\n","class Stacking ():\n","    def __init__(self, trained_models: list, meta_model, train_data: pd.DataFrame, test_data: pd.DataFrame, window_size : int = 10):\n","        self.trained_models = trained_models\n","        self.meta_model = self.get_meta_model(meta_model) #accept a string as an input\n","        self.train_data = pd.DataFrame(train_data)\n","        self.test_data = pd.DataFrame(test_data)\n","        self.MinMaxscaler = MinMaxScaler(feature_range=(-1, 1))\n","        self.Robustscaler = RobustScaler()\n","        # self.error_scaler = MinMaxScaler(feature_range=(-1, 1))\n","        self.window_size = window_size\n","        self.oof_predictions = []\n","        self.test_error_predictions = []\n","        self.device = self.get_device()\n","        self.lock = threading.Lock()\n","        self.ensemble_preds = None\n","        self.tscv = OverlappingTimeSeriesSplit(n_splits=5, window_size=self.window_size) #TimeSeriesSplit object to generate time ware time splits\n","        self.fist_fold_idx = None\n","        self.test_predictions = None\n","\n","\n","        #if there are deep learning models\n","        if any(isinstance(model, torch.nn.Module) for model in trained_models):\n","            self.train_loader, self.test_loader = self.data_preprocessing_deep_learning(self.train_data, self.test_data, self.MinMaxscaler, self.window_size)\n","\n","        #if there are machine learning models\n","        if any(not isinstance(model, torch.nn.Module) for model in trained_models):\n","            self.X_train, self.y_train, self.X_test, self.y_test = self.data_preprocessing_machine_learning(self.train_data, self.test_data, self.Robustscaler, self.window_size)\n","\n","\n","\n","    def feature_engineer(self, volatility, prediction):\n","        #target\n","        volatility['error_volatility'] = volatility\n","\n","\n","        #covariates\n","        # dataset['prediction'] = pd.DataFrame(prediction)\n","        # dataset['predicted volatility'] = dataset['prediction'].ewm(span=10, adjust=False).std().fillna(0)\n","\n","        # for lag in range(1, 5):\n","        for lag in range(1, 3):\n","            volatility[f'lag_{lag}'] = volatility['error_volatility'].shift(lag).fillna(0)\n","\n","\n","\n","    def get_meta_model(self, meta_model):\n","        try:\n","            if meta_model == \"linear_regression\":\n","                from sklearn.linear_model import LinearRegression\n","                return LinearRegression()\n","\n","            elif meta_model == \"ridge\":\n","                from sklearn.linear_model import Ridge\n","                return Ridge()\n","\n","            elif meta_model == \"gradient_boosting\":\n","                from sklearn.ensemble import GradientBoostingRegressor\n","                return GradientBoostingRegressor()\n","\n","            elif meta_model == \"random_forest\":\n","                from sklearn.ensemble import RandomForestRegressor\n","                return RandomForestRegressor()\n","\n","            else:\n","                raise ValueError(f\"Unsupported meta model: {meta_model}\")\n","\n","        except Exception as e:\n","            print(f\"Meta model error: {e}\")\n","            raise e\n","\n","\n","\n","    def sliding_windows(self, data, seq_length): #helper function to genereate sliding windows\n","        X, y = [], []\n","\n","        for i in range(len(data) - seq_length ):\n","            _x = data.iloc[i:i+seq_length]\n","            _y = data.iloc[i + seq_length]\n","            X.append(_x)\n","            y.append(_y)\n","\n","        return np.array(X), np.array(y)\n","\n","\n","    def get_device(self):\n","        if torch.cuda.is_available():\n","            device = 'cuda'\n","        else:\n","            device = 'cpu'\n","\n","        return device\n","\n","\n","    @staticmethod\n","    def to_device(data, device):\n","        if isinstance(data, (list, tuple)):\n","            return [to_device(x, device) for x in data]\n","        elif isinstance(data, torch.Tensor):  # Only move tensors to the device\n","            return data.to(device, non_blocking=True)\n","        else:\n","            return data  # For non-tensor types (e.g., strings), return as is\n","\n","\n","    @torch.no_grad()\n","    def inference(self, model, data_loader):\n","        model.eval()  # Set the model to evaluation mode\n","        predictions = []\n","        actuals = []\n","\n","        for batch in data_loader:\n","            input, output = batch  # Unpack your batch into input and output\n","            outputs = model(input) # Perform the forward pass\n","\n","            # Move outputs and actuals back to CPU and append to lists\n","            predictions.append(outputs.cpu())\n","            actuals.append(output.cpu())\n","\n","        # Concatenate all predictions and actuals into single tensors\n","        predictions = torch.cat(predictions, dim=0)\n","        actuals = torch.cat(actuals, dim=0)\n","\n","        return predictions, actuals\n","\n","\n","    def data_preprocessing_deep_learning (self, train_data, test_data, scaler, time_window):\n","        #scale data\n","        train_data_scaled = pd.DataFrame(scaler.fit_transform(train_data))\n","        test_data_scaled = pd.DataFrame(scaler.transform(test_data))\n","\n","        #create sliding windows\n","        X_train, y_train = self.sliding_windows(train_data_scaled, time_window)\n","        X_test, y_test = self.sliding_windows(test_data_scaled, time_window)\n","\n","        #move data to tensors\n","        train_df = Custom_df(X_train, y_train)\n","        test_df = Custom_df(X_test, y_test)\n","\n","        #create dataloaders (to perform training/inference in batch)\n","        batch_size = 128\n","        num_workers = 2\n","        pin_memory = True if self.device == \"cuda\" else False\n","        train_loader = DataLoader(train_df, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory, persistent_workers=True)\n","        test_loader = DataLoader(test_df, batch_size=batch_size*2, shuffle=False, num_workers=num_workers, pin_memory=pin_memory, persistent_workers=True)\n","\n","        #move to device\n","        train_loader = DeviceDataLoader(train_loader, self.device)\n","        test_loader = DeviceDataLoader(test_loader, self.device)\n","\n","        return train_loader, test_loader\n","\n","\n","    def data_preprocessing_machine_learning(self, train_data, test_data, scaler, time_window):\n","        #convert data to numpy arrays\n","        if isinstance(train_data, torch.Tensor):\n","            train_data = train_data.numpy()\n","        if isinstance(test_data, torch.Tensor):\n","            test_data = test_data.numpy()\n","\n","        #scale data\n","\n","        if isinstance(train_data, pd.Series):\n","            train_data_scaled = pd.DataFrame(scaler.fit_transform(train_data.values.reshape(-1,1)))\n","            test_data_scaled = pd.DataFrame(scaler.transform(test_data.values.reshape(-1,1)))\n","\n","        else:\n","            train_data_scaled = pd.DataFrame(scaler.fit_transform(train_data))\n","            test_data_scaled = pd.DataFrame(scaler.transform(test_data))\n","\n","        #create sliding windows\n","        X_train, y_train = self.sliding_windows(train_data_scaled, time_window)\n","        X_test, y_test = self.sliding_windows(test_data_scaled, time_window)\n","\n","        # Flatten the sliding windows\n","        X_train = X_train.reshape(X_train.shape[0], -1)\n","        X_test = X_test.reshape(X_test.shape[0], -1)\n","\n","        return X_train, y_train, X_test, y_test\n","\n","\n","\n","\n","    def k_fold_model_prediction(self, model):\n","        try:\n","            if isinstance(model, LinearBoostRegressor): #Linear Boost model predictions\n","\n","                #compute k fold out of sample predictions\n","                oof_predictions = np.zeros(len(self.train_data))\n","\n","                with self.lock:\n","                    for fold, (train_idx, test_idx) in enumerate(self.tscv.split(range(len(self.train_data)))):\n","\n","                        #save first fold (unused)\n","                        if fold == 0:\n","                            self.fist_fold_idx = test_idx[self.window_size]\n","\n","\n","                        #data preprocessing\n","                        train_df, test_df = self.train_data.iloc[train_idx], self.train_data.iloc[test_idx]\n","\n","                        #X,y\n","                        scaler = RobustScaler()\n","                        X_train, y_train, X_test, y_test = self.data_preprocessing_machine_learning(train_df, test_df, scaler, self.window_size)\n","\n","                        #predict\n","                        test_prediction = model.predict(X_test)\n","\n","                        #inverse scale\n","                        test_prediction = scaler.inverse_transform(test_prediction.reshape(-1, 1))\n","\n","                        # Store predictions in the correct indices\n","                        oof_predictions[test_idx[self.window_size:]] = test_prediction.flatten()\n","\n","\n","                    #visualize predictions\n","                    plt.figure(figsize=(12, 6))\n","                    sns.lineplot(x= self.train_data[self.window_size:].index, y = self.train_data[self.window_size:].squeeze(), label=\"Train Data\")\n","                    plt.title('Out of sample XGBoost Predictions on Train set')\n","                    sns.lineplot(x= self.train_data[self.fist_fold_idx:].index, y=oof_predictions[self.fist_fold_idx:].squeeze(), label=\"Out of sample XGBoost predictions on Train set\")\n","                    plt.xlabel('Date')\n","                    plt.ylabel('Close price')\n","                    plt.xticks(rotation=45)  # Rotate x-axis labels\n","                    plt.tight_layout() # Automatically adjusts the layout\n","                    plt.legend()\n","                    plt.show()\n","\n","                    # print(self.train_data[fist_fold_idx:].shape)\n","                    # print(oof_predictions.shape)\n","\n","                    r2 = r2_score(self.train_data[self.fist_fold_idx:].values, oof_predictions[self.fist_fold_idx:])\n","                    print(f\"R2 score: {r2}\")\n","\n","\n","                return (oof_predictions[self.fist_fold_idx:])\n","\n","\n","            elif isinstance(model, torch.nn.Module):\n","\n","\n","                #compute k fold out of sample predictions\n","                oof_predictions = np.zeros(len(self.train_data))\n","\n","                with self.lock:\n","                    for fold, (train_idx, test_idx) in enumerate(self.tscv.split(range(len(self.train_data)))):\n","\n","                        #save first fold (unused)\n","                        if fold == 0:\n","                            self.fist_fold_idx = test_idx[self.window_size]\n","\n","                        #data preprocessing\n","                        train_df, test_df = self.train_data.iloc[train_idx], self.train_data.iloc[test_idx]\n","\n","                        #X,y\n","                        scaler = MinMaxScaler(feature_range=(-1, 1))\n","                        train_loader, test_loader = self.data_preprocessing_deep_learning(train_df, test_df, scaler, self.window_size)\n","\n","                        #predict\n","                        test_prediction, test_actual = self.inference(model, test_loader)\n","\n","                        #inverse scale\n","                        test_prediction = scaler.inverse_transform(test_prediction)\n","\n","                        # Store predictions in the correct indices\n","                        oof_predictions[test_idx[self.window_size:]] = test_prediction.flatten()\n","\n","\n","\n","                    #visualize predictions\n","                    plt.figure(figsize=(12, 6))\n","                    sns.lineplot(x= self.train_data[self.window_size:].index, y = self.train_data[self.window_size:].squeeze(), label=\"Train Data\")\n","                    sns.lineplot(x= self.train_data[self.fist_fold_idx:].index, y=oof_predictions[self.fist_fold_idx:].squeeze(), label=f\"Out of sample {model.__class__.__name__} predictions on Train set\")\n","                    plt.title(f\"Out of sample {model.__class__.__name__} predictions on Train set\")\n","                    plt.xlabel('Date')\n","                    plt.ylabel('Close price')\n","                    plt.xticks(rotation=45)  # Rotate x-axis labels\n","                    plt.tight_layout() # Automatically adjusts the layout\n","                    plt.legend()\n","                    plt.show()\n","\n","                    r2 = r2_score(self.train_data[self.fist_fold_idx:].values, oof_predictions[self.fist_fold_idx:])\n","                    print(f\"R2 score: {r2}\")\n","\n","\n","                    # Explicit cleanup\n","                    del train_loader, test_loader\n","                    torch.cuda.empty_cache()\n","\n","\n","\n","                return (oof_predictions[self.fist_fold_idx:])\n","\n","\n","            else:\n","                raise ValueError(f\"Unsupported base model type: {type(model)}\")\n","\n","\n","        except Exception as e:\n","            print(f\"Error processing model {model}: {e}\")\n","            raise e\n","\n","\n","\n","\n","    def model_prediction(self, model):\n","        try:\n","            if isinstance(model, LinearBoostRegressor): #Linear Boost model predictions\n","\n","                with self.lock:\n","                    test_prediction = model.predict(self.X_test)\n","\n","                    #inverse scale\n","                    test_prediction = self.Robustscaler.inverse_transform(test_prediction.reshape(-1, 1))\n","\n","                    #visualize predictions\n","                    plt.figure(figsize=(12, 6))\n","                    sns.lineplot(x= self.test_data[self.window_size:].index, y = self.test_data[self.window_size:].squeeze(), label=\"Train Data\")\n","                    plt.title('XGBoost Predictions on Test set')\n","                    sns.lineplot(x= self.test_data[self.window_size:].index, y=test_prediction.squeeze(), label=\"Out of sample XGBoost predictions on Train set\")\n","                    plt.xlabel('Date')\n","                    plt.ylabel('Close price')\n","                    plt.xticks(rotation=45)  # Rotate x-axis labels\n","                    plt.tight_layout() # Automatically adjusts the layout\n","                    plt.legend()\n","                    plt.show()\n","\n","\n","                    r2 = r2_score(self.test_data[self.window_size:].values, test_prediction)\n","                    print(f\"R2 score: {r2}\")\n","\n","\n","                return (test_prediction)\n","\n","\n","            elif isinstance(model, torch.nn.Module):\n","                #train predictions\n","                with self.lock: #the lock prevents multiple threads from accessing shared data at the same time\n","                    test_prediction, test_actual = self.inference(model, self.test_loader)\n","\n","                    # Inverse Scale\n","                    test_prediction = self.MinMaxscaler.inverse_transform(test_prediction)\n","\n","                    #visualize predictions\n","                    plt.figure(figsize=(12, 6))\n","                    sns.lineplot(x= self.test_data[self.window_size:].index, y = self.test_data[self.window_size:].squeeze(), label=\"Train Data\")\n","                    sns.lineplot(x= self.test_data[self.window_size:].index, y=test_prediction.squeeze(), label=f\"{model.__class__.__name__} predictions on Test set\")\n","                    plt.title(f\"{model.__class__.__name__} predictions on Test set\")\n","                    plt.xlabel('Date')\n","                    plt.ylabel('Close price')\n","                    plt.xticks(rotation=45)  # Rotate x-axis labels\n","                    plt.tight_layout() # Automatically adjusts the layout\n","                    plt.legend()\n","                    plt.show()\n","\n","                    r2 = r2_score(self.test_data[self.window_size:].values, test_prediction)\n","                    print(f\"R2 score: {r2}\")\n","\n","                return (test_prediction)\n","\n","\n","            else:\n","                raise ValueError(f\"Unsupported base model type: {type(model)}\")\n","\n","\n","        except Exception as e:\n","            print(f\"Error processing model {model}: {e}\")\n","            raise e\n","\n","\n","    def predict(self):\n","        #parallel execution\n","        print(\"Out of fold predictions on train set\")\n","        with ThreadPoolExecutor() as executor: #it creates a pool of worker threads (the with statement ensures that the pool of threads is cleaned up automatically after the execution)\n","            results = list(executor.map(self.k_fold_model_prediction, self.trained_models)) #the task each thread will execute\n","\n","        # Combine oof predictions into a single array (meta features)\n","        self.oof_predictions = np.hstack([item.reshape(-1, 1) for item in results])\n","        # print(self.oof_predictions.shape)\n","\n","\n","        #train meta model\n","        self.meta_model.fit(self.oof_predictions, self.Robustscaler.inverse_transform(self.y_train[(self.fist_fold_idx -self.window_size):]))\n","        print(\"meta model trained\")\n","\n","\n","        #retrain model on all train set\n","        print(\"Prediction on test set\")\n","        with ThreadPoolExecutor() as executor: #it creates a pool of worker threads (the with statement ensures that the pool of threads is cleaned up automatically after the execution)\n","            test_results = list(executor.map(self.model_prediction, self.trained_models)) #the task each thread will execute\n","\n","        #stack test predictions\n","        self.test_predictions = np.hstack([item.reshape(-1, 1) for item in test_results])\n","        # print(self.test_predictions.shape)\n","\n","        #use meta model to generate final predictions\n","        final_preds = self.meta_model.predict(self.test_predictions)\n","\n","        return final_preds"],"metadata":{"id":"AXU2yw-nFT-E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#using the class\n","train_data = {}\n","test_data = {}\n","\n","trained_models = [Linear_booster_model, CNN_model, LSTM_model, LSTM_CNN_model, parallelCNNLSTM_model]\n","\n","for name , _ in datasets.items():\n","\n","    # if name == 'sp500':\n","    #     continue\n","\n","    # if name == 'eur_usd':\n","    #     continue\n","\n","    print(f\"ensembling predictions on {name} dataset:\")\n","\n","    meta_model = \"linear_regression\"\n","\n","    train_data[name] = val_residuals[name]\n","    train_data[name].index = pd.to_datetime(val_residuals[name].index)\n","\n","    test_data[name] = test_residuals[name]\n","    test_data[name].index = pd.to_datetime(test_residuals[name].index)\n","    window_size = 10\n","\n","\n","    #stacking\n","    stack = Stacking([trained_model[name] for trained_model in trained_models], meta_model, train_data[name], test_data[name], window_size) #k represent the number of model to select from the batch of models\n","    final_preds = stack.predict()\n","\n","\n","    #visualize\n","    ensemble_preds = pd.DataFrame(final_preds, index = test_data[name][window_size:].index)\n","\n","    plt.figure(figsize=(12, 6))\n","    plt.title(f\"Stacking ensemble predictions {name}\")\n","    sns.lineplot(x = ensemble_preds.index, y = ensemble_preds.squeeze(), label = \"ADE\", color = \"red\")\n","    # sns.lineplot(x = test_data[name][window_size*2:].index, y = test_data[name][window_size*2:].squeeze(), label = \"test set\", color = \"blue\")\n","    sns.lineplot(x = test_data[name][window_size:].index, y = test_data[name][window_size:].squeeze(), label = \"test set\", color = \"blue\")\n","    plt.ylabel('Close price residuals')\n","    plt.xlabel('Date')\n","    plt.xticks(rotation=45)  # Rotate x-axis labels\n","    plt.tight_layout() # Automatically adjusts the layout\n","    plt.legend()\n","    plt.show()\n","\n","\n","    #compute metrics\n","    # mse = mean_squared_error(test_data[name][window_size*2:], ensemble_preds)\n","    # rmse = np.sqrt(mse)\n","    # mae = mean_absolute_error(test_data[name][window_size*2:], ensemble_preds)\n","    # r2 = r2_score(test_data[name][window_size*2:], ensemble_preds)\n","    # mape = mean_absolute_percentage_error(test_data[name][window_size*2:], ensemble_preds)\n","    # smape_value = smape(test_data[name][window_size*2:], ensemble_preds)\n","\n","    mse = mean_squared_error(test_data[name][window_size:], ensemble_preds)\n","    rmse = np.sqrt(mse)\n","    mae = mean_absolute_error(test_data[name][window_size:], ensemble_preds)\n","    r2 = r2_score(test_data[name][window_size:], ensemble_preds)\n","    mape = mean_absolute_percentage_error(test_data[name][window_size:], ensemble_preds)\n","    smape_value = smape(test_data[name][window_size:], ensemble_preds)\n","\n","    print(f\"MSE: {mse}\")\n","    print(f\"RMSE: {rmse}\")\n","    print(f\"MAE: {mae}\")\n","    print(f\"r2:  {r2}\")\n","    print(f\"MAPE: {mape}\")\n","    print(f\"SMAPE: {smape_value}\")\n","\n","\n","    #combine ensemble predictions with ARIMA model\n","    final_preds = SARIMA_test_predictions[name][window_size:] + ensemble_preds.iloc[:,0]\n","\n","    #visualize\n","    plt.figure(figsize=(12, 6))\n","    plt.title(f\"SARIMA + ensemble predictions {name}\")\n","    sns.lineplot(x = final_preds.index, y = final_preds.squeeze(), label = \"SARIMA + ensemble predictions\", color = \"red\")\n","    sns.lineplot(x = Close_price[name].loc[final_preds.index].index, y = Close_price[name].loc[final_preds.index].squeeze(), label = \"test set\", color = \"blue\")\n","    plt.ylabel('Close price')\n","    plt.xlabel('Date')\n","    plt.xticks(rotation=45)  # Rotate x-axis labels\n","    plt.tight_layout() # Automatically adjusts the layout\n","    plt.legend()\n","    plt.show()\n","\n","\n","    #compute metrics\n","    mse = mean_squared_error(Close_price[name].loc[final_preds.index], final_preds)\n","    rmse = np.sqrt(mse)\n","    mae = mean_absolute_error(Close_price[name].loc[final_preds.index], final_preds)\n","    r2 = r2_score(Close_price[name].loc[final_preds.index], final_preds)\n","    mape = mean_absolute_percentage_error(Close_price[name].loc[final_preds.index], final_preds)\n","    smape_value = smape(Close_price[name].loc[final_preds.index], final_preds)\n","\n","    print(f\"MSE: {mse}\")\n","    print(f\"RMSE: {rmse}\")\n","    print(f\"MAE: {mae}\")\n","    print(f\"r2:  {r2}\")\n","    print(f\"MAPE: {mape}\")\n","    print(f\"SMAPE: {smape_value}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"18pLSKR1sAcsuwJm-Av28dLWZvtewy_6m"},"id":"Sb5ASg92Nb0q","outputId":"71733f6d-a832-4542-a778-e419db9244f3","executionInfo":{"status":"ok","timestamp":1740612931462,"user_tz":-60,"elapsed":40535,"user":{"displayName":"Andrea Damiano","userId":"07680645090793871056"}}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["Static Weighted Average"],"metadata":{"id":"plha0yI2PzdW"}},{"cell_type":"code","source":["#import libraries\n","import pandas as pd\n","import numpy as np\n","from google.colab import drive\n","import datetime\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","import statsmodels.api as sm\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, Subset\n","from joblib import Parallel, delayed, dump, load\n","import xgboost as xgb\n","import time\n","import yfinance as yf\n","import torch.nn.functional as F\n","from concurrent.futures import ThreadPoolExecutor\n","import threading\n","from sklearn.linear_model import LogisticRegression\n","# from sklearn.model_selection import TimeSeriesSplit\n","\n","#suppress warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","\n","class OverlappingTimeSeriesSplit():\n","    def __init__(self, n_splits=5, window_size=10):\n","        self.n_splits = n_splits\n","        self.window_size = window_size\n","\n","    def split(self, X):\n","        n_samples = len(X)\n","        fold_size = n_samples // (self.n_splits + 1)  # Ensure proper splitting\n","\n","        for i in range(1, self.n_splits + 1):  # Start from 1 to avoid empty train set\n","            train_end = i * fold_size\n","            train_idx = range(train_end)  # Train set includes all data before validation\n","\n","            val_start = max(0, train_end - self.window_size)\n","            val_end = train_end + fold_size\n","            val_idx = range(val_start, min(val_end, n_samples))  # Ensure within bounds\n","\n","            yield list(train_idx), list(val_idx)\n","\n","def smape(y_true, y_pred):\n","    y_true = np.array(y_true)\n","    y_pred = np.array(y_pred)\n","\n","    numerator = np.abs(y_pred - y_true)\n","    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n","    smape_value = np.mean(numerator / denominator)  # Renamed variable to avoid conflict\n","    return smape_value\n","\n","#deep learning meta model architecture\n","class LSTM(nn.Module):\n","\n","    def __init__(self, num_classes, input_size, hidden_size=512, num_layers=3, dropout=0.3):\n","        super().__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.num_classes = num_classes\n","\n","        # LSTM layers\n","        self.lstm = nn.LSTM(\n","            input_size=input_size,\n","            hidden_size=hidden_size,\n","            num_layers=num_layers,\n","            batch_first=True,\n","            dropout=dropout\n","        )\n","\n","        # Fully connected layers\n","        self.fc1 = nn.Linear(hidden_size, 256)\n","        self.fc2 = nn.Linear(256, 128)\n","        self.fc3 = nn.Linear(128, self.num_classes)\n","\n","        # Activation and dropout\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","    def forward(self, x):\n","        # LSTM output\n","        lstm_out, _ = self.lstm(x)\n","\n","        # Pass only the last output\n","        lstm_out = lstm_out[:, -1, :]\n","\n","        # Fully connected layers with ReLU and Dropout\n","        x = self.relu(self.fc1(lstm_out))\n","        x = self.dropout(x)\n","        x = self.relu(self.fc2(x))\n","        x = self.dropout(x)\n","        x = self.fc3(x)  # Final prediction\n","        return x\n","\n","\n","#deep learning meta model architecture\n","class ParallelCNN_LSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n","        super().__init__()\n","        self.cnn = nn.Sequential(\n","            nn.Conv1d(in_channels=input_size, out_channels=64, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool1d(kernel_size=2, stride=2),\n","            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool1d(kernel_size=2, stride=2),\n","            nn.Flatten(),\n","            nn.LazyLinear(out_features=128), #linear layer that automatically infer the input size\n","            nn.ReLU()\n","        )\n","        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n","        self.fc_lstm = nn.Linear(hidden_size, 128)\n","        self.fc = nn.Linear(128*2, num_classes)\n","\n","    def forward(self, x):\n","        #cnn takes input of shape (batch_size, channels, seq_len)\n","        x_cnn = x.permute(0, 2, 1)\n","        out_cnn = self.cnn(x_cnn)\n","        # lstm takes input of shape (batch_size, seq_len, input_size)\n","        out_lstm, _ = self.lstm(x)\n","        out_lstm = self.fc_lstm(out_lstm[:, -1, :])\n","        out = torch.cat([out_cnn, out_lstm], dim=1)\n","        out = self.fc(out)\n","        return out\n","\n","\n","class Custom_df(Dataset):\n","    def __init__(self, x, y):\n","        self.x = x\n","        self.y = y\n","\n","    def __getitem__(self, index): #this method allows to retrieve a specific sample from the dataset based on its index (the index is passed to this method)\n","        input = torch.tensor(np.array(self.x[index]), dtype=torch.float32) #convert np array into a tensor\n","        target = torch.tensor(np.array(self.y[index]), dtype=torch.float32)\n","        return input, target\n","\n","    def __len__(self):\n","        return len(self.x)\n","\n","\n","class DeviceDataLoader (): #receive a dataloader and move to the correct device\n","  def __init__(self, dl : DataLoader, device):\n","    self.dl = dl\n","    self.device = device\n","\n","  def __iter__(self):\n","    for batch in iter(self.dl): #iter will reset the iterator every time the __iter__ mathod gets called\n","      yield Weighted_Average.to_device(batch, device)\n","\n","  def __len__(self):\n","    return len(self.dl)\n","\n","\n","\n","class Weighted_Average ():\n","    def __init__(self, trained_models: list, meta_model, train_data: pd.DataFrame, test_data: pd.DataFrame, weights = None, window_size : int = 10, k = 5, temperature = 1):\n","        self.trained_models = trained_models\n","        self.meta_model = self.get_meta_model(meta_model) #accept a string as an input\n","        self.train_data = pd.DataFrame(train_data)\n","        self.test_data = pd.DataFrame(test_data)\n","        self.MinMaxscaler = MinMaxScaler(feature_range=(-1, 1))\n","        self.Robustscaler = RobustScaler()\n","        # self.error_scaler = MinMaxScaler(feature_range=(-1, 1))\n","        self.weights = weights\n","        self.window_size = window_size\n","        self.oof_predictions = []\n","        self.test_error_predictions = []\n","        self.device = self.get_device()\n","        self.lock = threading.Lock()\n","        self.ensemble_preds = None\n","        self.tscv = OverlappingTimeSeriesSplit(n_splits=5, window_size=self.window_size) #TimeSeriesSplit object to generate time ware time splits\n","        self.fist_fold_idx = None\n","        self.test_predictions = None\n","        self.k = k\n","        self.temperature = temperature\n","\n","\n","        #if there are deep learning models\n","        if any(isinstance(model, torch.nn.Module) for model in trained_models):\n","            self.train_loader, self.test_loader = self.data_preprocessing_deep_learning(self.train_data, self.test_data, self.MinMaxscaler, self.window_size)\n","\n","        #if there are machine learning models\n","        if any(not isinstance(model, torch.nn.Module) for model in trained_models):\n","            self.X_train, self.y_train, self.X_test, self.y_test = self.data_preprocessing_machine_learning(self.train_data, self.test_data, self.Robustscaler, self.window_size)\n","\n","\n","\n","    def feature_engineer(self, volatility, prediction):\n","        #target\n","        volatility['error_volatility'] = volatility\n","\n","\n","        #covariates\n","        # dataset['prediction'] = pd.DataFrame(prediction)\n","        # dataset['predicted volatility'] = dataset['prediction'].ewm(span=10, adjust=False).std().fillna(0)\n","\n","        # for lag in range(1, 5):\n","        for lag in range(1, 3):\n","            volatility[f'lag_{lag}'] = volatility['error_volatility'].shift(lag).fillna(0)\n","\n","\n","\n","    def get_meta_model(self, meta_model):\n","        try:\n","            if meta_model == \"linear_regression\":\n","                from sklearn.linear_model import LinearRegression\n","                return LinearRegression()\n","\n","            elif meta_model == \"ridge\":\n","                from sklearn.linear_model import Ridge\n","                return Ridge()\n","\n","            elif meta_model == \"gradient_boosting\":\n","                from sklearn.ensemble import GradientBoostingRegressor\n","                return GradientBoostingRegressor()\n","\n","            elif meta_model == \"random_forest\":\n","                from sklearn.ensemble import RandomForestRegressor\n","                return RandomForestRegressor()\n","\n","            else:\n","                raise ValueError(f\"Unsupported meta model: {meta_model}\")\n","\n","        except Exception as e:\n","            print(f\"Meta model error: {e}\")\n","            raise e\n","\n","\n","\n","    def sliding_windows(self, data, seq_length): #helper function to genereate sliding windows\n","        X, y = [], []\n","\n","        for i in range(len(data) - seq_length ):\n","            _x = data.iloc[i:i+seq_length]\n","            _y = data.iloc[i + seq_length]\n","            X.append(_x)\n","            y.append(_y)\n","\n","        return np.array(X), np.array(y)\n","\n","\n","    def get_device(self):\n","        if torch.cuda.is_available():\n","            device = 'cuda'\n","        else:\n","            device = 'cpu'\n","\n","        return device\n","\n","\n","    @staticmethod\n","    def to_device(data, device):\n","        if isinstance(data, (list, tuple)):\n","            return [to_device(x, device) for x in data]\n","        elif isinstance(data, torch.Tensor):  # Only move tensors to the device\n","            return data.to(device, non_blocking=True)\n","        else:\n","            return data  # For non-tensor types (e.g., strings), return as is\n","\n","\n","    @torch.no_grad()\n","    def inference(self, model, data_loader):\n","        model.eval()  # Set the model to evaluation mode\n","        predictions = []\n","        actuals = []\n","\n","        for batch in data_loader:\n","            input, output = batch  # Unpack your batch into input and output\n","            outputs = model(input) # Perform the forward pass\n","\n","            # Move outputs and actuals back to CPU and append to lists\n","            predictions.append(outputs.cpu())\n","            actuals.append(output.cpu())\n","\n","        # Concatenate all predictions and actuals into single tensors\n","        predictions = torch.cat(predictions, dim=0)\n","        actuals = torch.cat(actuals, dim=0)\n","\n","        return predictions, actuals\n","\n","\n","    def data_preprocessing_deep_learning (self, train_data, test_data, scaler, time_window):\n","        #scale data\n","        train_data_scaled = pd.DataFrame(scaler.fit_transform(train_data))\n","        test_data_scaled = pd.DataFrame(scaler.transform(test_data))\n","\n","        #create sliding windows\n","        X_train, y_train = self.sliding_windows(train_data_scaled, time_window)\n","        X_test, y_test = self.sliding_windows(test_data_scaled, time_window)\n","\n","        #move data to tensors\n","        train_df = Custom_df(X_train, y_train)\n","        test_df = Custom_df(X_test, y_test)\n","\n","        #create dataloaders (to perform training/inference in batch)\n","        batch_size = 128\n","        num_workers = 2\n","        pin_memory = True if self.device == \"cuda\" else False\n","        train_loader = DataLoader(train_df, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory, persistent_workers=True)\n","        test_loader = DataLoader(test_df, batch_size=batch_size*2, shuffle=False, num_workers=num_workers, pin_memory=pin_memory, persistent_workers=True)\n","\n","        #move to device\n","        train_loader = DeviceDataLoader(train_loader, self.device)\n","        test_loader = DeviceDataLoader(test_loader, self.device)\n","\n","        return train_loader, test_loader\n","\n","\n","    def data_preprocessing_machine_learning(self, train_data, test_data, scaler, time_window):\n","        #convert data to numpy arrays\n","        if isinstance(train_data, torch.Tensor):\n","            train_data = train_data.numpy()\n","        if isinstance(test_data, torch.Tensor):\n","            test_data = test_data.numpy()\n","\n","        #scale data\n","\n","        if isinstance(train_data, pd.Series):\n","            train_data_scaled = pd.DataFrame(scaler.fit_transform(train_data.values.reshape(-1,1)))\n","            test_data_scaled = pd.DataFrame(scaler.transform(test_data.values.reshape(-1,1)))\n","\n","        else:\n","            train_data_scaled = pd.DataFrame(scaler.fit_transform(train_data))\n","            test_data_scaled = pd.DataFrame(scaler.transform(test_data))\n","\n","        #create sliding windows\n","        X_train, y_train = self.sliding_windows(train_data_scaled, time_window)\n","        X_test, y_test = self.sliding_windows(test_data_scaled, time_window)\n","\n","        # Flatten the sliding windows\n","        X_train = X_train.reshape(X_train.shape[0], -1)\n","        X_test = X_test.reshape(X_test.shape[0], -1)\n","\n","        return X_train, y_train, X_test, y_test\n","\n","\n","\n","\n","    def k_fold_model_prediction(self, model):\n","        try:\n","            if isinstance(model, LinearBoostRegressor): #Linear Boost model predictions\n","\n","                #compute k fold out of sample predictions\n","                oof_predictions = np.zeros(len(self.train_data))\n","\n","                with self.lock:\n","                    for fold, (train_idx, test_idx) in enumerate(self.tscv.split(range(len(self.train_data)))):\n","\n","                        #save first fold (unused)\n","                        if fold == 0:\n","                            self.fist_fold_idx = test_idx[self.window_size]\n","\n","\n","                        #data preprocessing\n","                        train_df, test_df = self.train_data.iloc[train_idx], self.train_data.iloc[test_idx]\n","\n","                        #X,y\n","                        scaler = RobustScaler()\n","                        X_train, y_train, X_test, y_test = self.data_preprocessing_machine_learning(train_df, test_df, scaler, self.window_size)\n","\n","                        #predict\n","                        test_prediction = model.predict(X_test)\n","\n","                        #inverse scale\n","                        test_prediction = scaler.inverse_transform(test_prediction.reshape(-1, 1))\n","\n","                        # Store predictions in the correct indices\n","                        oof_predictions[test_idx[self.window_size:]] = test_prediction.flatten()\n","\n","\n","                    #visualize predictions\n","                    plt.figure(figsize=(12, 6))\n","                    sns.lineplot(x= self.train_data[self.window_size:].index, y = self.train_data[self.window_size:].squeeze(), label=\"Train Data\")\n","                    plt.title('Out of sample XGBoost Predictions on Train set')\n","                    sns.lineplot(x= self.train_data[self.fist_fold_idx:].index, y=oof_predictions[self.fist_fold_idx:].squeeze(), label=\"Out of sample XGBoost predictions on Train set\")\n","                    plt.xlabel('Date')\n","                    plt.ylabel('Close price')\n","                    plt.xticks(rotation=45)  # Rotate x-axis labels\n","                    plt.tight_layout() # Automatically adjusts the layout\n","                    plt.legend()\n","                    plt.show()\n","\n","                    # print(self.train_data[fist_fold_idx:].shape)\n","                    # print(oof_predictions.shape)\n","\n","                    r2 = r2_score(self.train_data[self.fist_fold_idx:].values, oof_predictions[self.fist_fold_idx:])\n","                    print(f\"R2 score: {r2}\")\n","\n","\n","                return (oof_predictions[self.fist_fold_idx:], r2)\n","\n","\n","            elif isinstance(model, torch.nn.Module):\n","\n","\n","                #compute k fold out of sample predictions\n","                oof_predictions = np.zeros(len(self.train_data))\n","\n","                with self.lock:\n","                    for fold, (train_idx, test_idx) in enumerate(self.tscv.split(range(len(self.train_data)))):\n","\n","                        #save first fold (unused)\n","                        if fold == 0:\n","                            self.fist_fold_idx = test_idx[self.window_size]\n","\n","                        #data preprocessing\n","                        train_df, test_df = self.train_data.iloc[train_idx], self.train_data.iloc[test_idx]\n","\n","                        #X,y\n","                        scaler = MinMaxScaler(feature_range=(-1, 1))\n","                        train_loader, test_loader = self.data_preprocessing_deep_learning(train_df, test_df, scaler, self.window_size)\n","\n","                        #predict\n","                        test_prediction, test_actual = self.inference(model, test_loader)\n","\n","                        #inverse scale\n","                        test_prediction = scaler.inverse_transform(test_prediction)\n","\n","                        # Store predictions in the correct indices\n","                        oof_predictions[test_idx[self.window_size:]] = test_prediction.flatten()\n","\n","\n","\n","                    #visualize predictions\n","                    plt.figure(figsize=(12, 6))\n","                    sns.lineplot(x= self.train_data[self.window_size:].index, y = self.train_data[self.window_size:].squeeze(), label=\"Train Data\")\n","                    sns.lineplot(x= self.train_data[self.fist_fold_idx:].index, y=oof_predictions[self.fist_fold_idx:].squeeze(), label=f\"Out of sample {model.__class__.__name__} predictions on Train set\")\n","                    plt.title(f\"Out of sample {model.__class__.__name__} predictions on Train set\")\n","                    plt.xlabel('Date')\n","                    plt.ylabel('Close price')\n","                    plt.xticks(rotation=45)  # Rotate x-axis labels\n","                    plt.tight_layout() # Automatically adjusts the layout\n","                    plt.legend()\n","                    plt.show()\n","\n","                    r2 = r2_score(self.train_data[self.fist_fold_idx:].values, oof_predictions[self.fist_fold_idx:])\n","                    print(f\"R2 score: {r2}\")\n","\n","\n","                    # Explicit cleanup\n","                    del train_loader, test_loader\n","                    torch.cuda.empty_cache()\n","\n","\n","\n","                return (oof_predictions[self.fist_fold_idx:], r2)\n","\n","\n","            else:\n","                raise ValueError(f\"Unsupported base model type: {type(model)}\")\n","\n","\n","        except Exception as e:\n","            print(f\"Error processing model {model}: {e}\")\n","            raise e\n","\n","\n","\n","\n","    def model_prediction(self, model):\n","        try:\n","            if isinstance(model, LinearBoostRegressor): #Linear Boost model predictions\n","\n","                with self.lock:\n","                    test_prediction = model.predict(self.X_test)\n","\n","                    #inverse scale\n","                    test_prediction = self.Robustscaler.inverse_transform(test_prediction.reshape(-1, 1))\n","\n","                    #visualize predictions\n","                    plt.figure(figsize=(12, 6))\n","                    sns.lineplot(x= self.test_data[self.window_size:].index, y = self.test_data[self.window_size:].squeeze(), label=\"Train Data\")\n","                    plt.title('XGBoost Predictions on Test set')\n","                    sns.lineplot(x= self.test_data[self.window_size:].index, y=test_prediction.squeeze(), label=\"Out of sample XGBoost predictions on Train set\")\n","                    plt.xlabel('Date')\n","                    plt.ylabel('Close price')\n","                    plt.xticks(rotation=45)  # Rotate x-axis labels\n","                    plt.tight_layout() # Automatically adjusts the layout\n","                    plt.legend()\n","                    plt.show()\n","\n","\n","                    r2 = r2_score(self.test_data[self.window_size:].values, test_prediction)\n","                    print(f\"R2 score: {r2}\")\n","\n","\n","                return (test_prediction, r2)\n","\n","\n","            elif isinstance(model, torch.nn.Module):\n","                #train predictions\n","                with self.lock: #the lock prevents multiple threads from accessing shared data at the same time\n","                    test_prediction, test_actual = self.inference(model, self.test_loader)\n","\n","                    # Inverse Scale\n","                    test_prediction = self.MinMaxscaler.inverse_transform(test_prediction)\n","\n","                    #visualize predictions\n","                    plt.figure(figsize=(12, 6))\n","                    sns.lineplot(x= self.test_data[self.window_size:].index, y = self.test_data[self.window_size:].squeeze(), label=\"Train Data\")\n","                    sns.lineplot(x= self.test_data[self.window_size:].index, y=test_prediction.squeeze(), label=f\"{model.__class__.__name__} predictions on Test set\")\n","                    plt.title(f\"{model.__class__.__name__} predictions on Test set\")\n","                    plt.xlabel('Date')\n","                    plt.ylabel('Close price')\n","                    plt.xticks(rotation=45)  # Rotate x-axis labels\n","                    plt.tight_layout() # Automatically adjusts the layout\n","                    plt.legend()\n","                    plt.show()\n","\n","                    r2 = r2_score(self.test_data[self.window_size:].values, test_prediction)\n","                    print(f\"R2 score: {r2}\")\n","\n","                return (test_prediction, r2)\n","\n","\n","            else:\n","                raise ValueError(f\"Unsupported base model type: {type(model)}\")\n","\n","\n","        except Exception as e:\n","            print(f\"Error processing model {model}: {e}\")\n","            raise e\n","\n","\n","    def predict(self):\n","\n","        #k fold train predictions to infer model weights\n","        if (self.weights == None):\n","            print(\"k fold train predictions\")\n","            with ThreadPoolExecutor() as executor: #it creates a pool of worker threads (the with statement ensures that the pool of threads is cleaned up automatically after the execution)\n","                k_fold_results = list(executor.map(self.k_fold_model_prediction, self.trained_models)) #the task each thread will execute\n","\n","            r2 = np.hstack([item[1] for item in k_fold_results])\n","            r2 = torch.tensor(r2, dtype=torch.float32).detach()\n","            print('r2')\n","            print(r2)\n","\n","            #filter the 3 best r2\n","            best_r2, indices = torch.topk(r2, k=self.k, largest=False, sorted=False)\n","            print(\"best k errors\")\n","            print(best_r2)\n","\n","\n","            #normalize the errors\n","            std = best_r2.std(dim=0, keepdim=True) + 1e-8 #for numerical stability (to avoid division by zero)\n","            print(\"std\")\n","            print(std[:10])\n","\n","            mean = best_r2.mean(dim=0, keepdim=True)\n","            print(\"mean\")\n","            print(mean[:10])\n","\n","            normalized_r2 = (r2 - mean) / std\n","            print(\"unmasked normalized errors\")\n","            print(normalized_r2)\n","\n","\n","            #apply mask\n","            mask = torch.zeros_like(r2, dtype=torch.bool)\n","            mask.scatter_(dim=0, index=indices, value=True)\n","            normalized_r2[~mask] = float('inf')\n","            print(\"normalized errors\")\n","            print(normalized_r2)\n","\n","\n","            #apply softmax\n","            self.weights = F.softmax(normalized_r2 * self.temperature, dim=0)\n","\n","            print('weights')\n","            print(self.weights)\n","\n","\n","\n","        #retrain model on all train set\n","        print(\"Weighted prediction on test set\")\n","        with ThreadPoolExecutor() as executor: #it creates a pool of worker threads (the with statement ensures that the pool of threads is cleaned up automatically after the execution)\n","            test_results = list(executor.map(self.model_prediction, self.trained_models)) #the task each thread will execute\n","\n","        #stack test predictions\n","        self.test_predictions = np.hstack([item[0].reshape(-1, 1) for item in test_results])\n","        # print(self.test_predictions.shape)\n","\n","\n","        #weighted average of the predictions\n","        final_preds = np.average(self.test_predictions, axis=1, weights=self.weights)\n","\n","        return final_preds"],"metadata":{"id":"VM5lzX7nP7Ah"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#using the class\n","train_data = {}\n","test_data = {}\n","\n","trained_models = [Linear_booster_model, CNN_model, LSTM_model, LSTM_CNN_model, parallelCNNLSTM_model]\n","\n","for name , _ in datasets.items():\n","\n","    # if name == 'sp500':\n","    #     continue\n","\n","    # if name == 'eur_usd':\n","    #     continue\n","\n","    print(f\"ensembling predictions on {name} dataset:\")\n","\n","    meta_model = \"linear_regression\"\n","    # weights = [0.2, 0.2, 0.2, 0.2, 0.2]\n","    weights = None\n","\n","    train_data[name] = val_residuals[name]\n","    train_data[name].index = pd.to_datetime(val_residuals[name].index)\n","\n","    test_data[name] = test_residuals[name]\n","    test_data[name].index = pd.to_datetime(test_residuals[name].index)\n","    window_size = 10\n","    k = 5\n","    temperature = 1\n","\n","\n","    #stacking\n","    ensemble = Weighted_Average([trained_model[name] for trained_model in trained_models], meta_model, train_data[name], test_data[name], weights, window_size, k, temperature) #k represent the number of model to select from the batch of models\n","    final_preds = ensemble.predict()\n","\n","\n","    #visualize\n","    ensemble_preds = pd.DataFrame(final_preds, index = test_data[name][window_size:].index)\n","\n","    plt.figure(figsize=(12, 6))\n","    plt.title(f\"Weighted average ensemble predictions {name}\")\n","    sns.lineplot(x = ensemble_preds.index, y = ensemble_preds.squeeze(), label = \"ADE\", color = \"red\")\n","    # sns.lineplot(x = test_data[name][window_size*2:].index, y = test_data[name][window_size*2:].squeeze(), label = \"test set\", color = \"blue\")\n","    sns.lineplot(x = test_data[name][window_size:].index, y = test_data[name][window_size:].squeeze(), label = \"test set\", color = \"blue\")\n","    plt.ylabel('Close price residuals')\n","    plt.xlabel('Date')\n","    plt.xticks(rotation=45)  # Rotate x-axis labels\n","    plt.tight_layout() # Automatically adjusts the layout\n","    plt.legend()\n","    plt.show()\n","\n","\n","    #compute metrics\n","    # mse = mean_squared_error(test_data[name][window_size*2:], ensemble_preds)\n","    # rmse = np.sqrt(mse)\n","    # mae = mean_absolute_error(test_data[name][window_size*2:], ensemble_preds)\n","    # r2 = r2_score(test_data[name][window_size*2:], ensemble_preds)\n","    # mape = mean_absolute_percentage_error(test_data[name][window_size*2:], ensemble_preds)\n","    # smape_value = smape(test_data[name][window_size*2:], ensemble_preds)\n","\n","    mse = mean_squared_error(test_data[name][window_size:], ensemble_preds)\n","    rmse = np.sqrt(mse)\n","    mae = mean_absolute_error(test_data[name][window_size:], ensemble_preds)\n","    r2 = r2_score(test_data[name][window_size:], ensemble_preds)\n","    mape = mean_absolute_percentage_error(test_data[name][window_size:], ensemble_preds)\n","    smape_value = smape(test_data[name][window_size:], ensemble_preds)\n","\n","    print(f\"MSE: {mse}\")\n","    print(f\"RMSE: {rmse}\")\n","    print(f\"MAE: {mae}\")\n","    print(f\"r2:  {r2}\")\n","    print(f\"MAPE: {mape}\")\n","    print(f\"SMAPE: {smape_value}\")\n","\n","\n","    #combine ensemble predictions with ARIMA model\n","    final_preds = SARIMA_test_predictions[name][window_size:] + ensemble_preds.iloc[:,0]\n","\n","    #visualize\n","    plt.figure(figsize=(12, 6))\n","    plt.title(f\"SARIMA + ensemble predictions {name}\")\n","    sns.lineplot(x = final_preds.index, y = final_preds.squeeze(), label = \"SARIMA + ensemble predictions\", color = \"red\")\n","    sns.lineplot(x = Close_price[name].loc[final_preds.index].index, y = Close_price[name].loc[final_preds.index].squeeze(), label = \"test set\", color = \"blue\")\n","    plt.ylabel('Close price')\n","    plt.xlabel('Date')\n","    plt.xticks(rotation=45)  # Rotate x-axis labels\n","    plt.tight_layout() # Automatically adjusts the layout\n","    plt.legend()\n","    plt.show()\n","\n","\n","    #compute metrics\n","    mse = mean_squared_error(Close_price[name].loc[final_preds.index], final_preds)\n","    rmse = np.sqrt(mse)\n","    mae = mean_absolute_error(Close_price[name].loc[final_preds.index], final_preds)\n","    r2 = r2_score(Close_price[name].loc[final_preds.index], final_preds)\n","    mape = mean_absolute_percentage_error(Close_price[name].loc[final_preds.index], final_preds)\n","    smape_value = smape(Close_price[name].loc[final_preds.index], final_preds)\n","\n","    print(f\"MSE: {mse}\")\n","    print(f\"RMSE: {rmse}\")\n","    print(f\"MAE: {mae}\")\n","    print(f\"r2:  {r2}\")\n","    print(f\"MAPE: {mape}\")\n","    print(f\"SMAPE: {smape_value}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1bp7p_XTkYFNRPUc5aZ96gM_cqz7tIsQX"},"id":"nXPx7Rq0Q4RQ","executionInfo":{"status":"ok","timestamp":1740613113519,"user_tz":-60,"elapsed":38721,"user":{"displayName":"Andrea Damiano","userId":"07680645090793871056"}},"outputId":"785b3dff-a9e1-449f-9349-f7e1c3c0cb4b"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}],"metadata":{"colab":{"collapsed_sections":["vUfBVn182zxU","kUTM7c1K61Gw"],"provenance":[],"authorship_tag":"ABX9TyNx2vblrm0Xd5zIHI1uPW26"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}